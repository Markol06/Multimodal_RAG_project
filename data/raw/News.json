[
  {
    "issue": 312,
    "title": "White House Resets U.S. AI Policy",
    "url": "https://www.deeplearning.ai/the-batch/issue-312/",
    "content": "President Trump set forth principles of an aggressive national AI policy, and he moved to implement them through an action plan and executive orders.\nWhat’s new: In “ Winning the Race: America’s AI Action Plan ,” the White House outlines a trio of near-term goals for AI in the United States: (i) stimulate innovation, (ii) build infrastructure, and (iii) establish global leadership. As initial steps in these directions, the president directed the federal government to (a) procure only “ideologically neutral” AI models, (b) accelerate permitting of data-center construction, and (c) promote exports of AI technology.\nHow it works: Rather than advocating for legislation or legal challenges, the plan focuses on actions the executive branch of government can take on its own. President Trump had ordered technology advisor Michael Kratsios, AI advisor David Sacks, and national security advisor Marco Rubio to make a plan to “sustain and enhance America’s global AI dominance” within days of starting his current term. Senior policy advisors Dean Ball and Sriram Krishnan, among others, also played key roles.\nStimulate innovation: The plan would support open-source and open-weights software by boosting U.S. developers’ access to processing power and driving adoption of open models by small and medium-size businesses. It calls for the U.S. to build scientific datasets; invest in interpretability, control, robustness, and evaluations; and promote AI in defense applications. In addition, the federal government will support the development of AI skills in its funding of education and workforce training. Moreover, in a speech, Trump said he wants AI companies to be allowed to use copyrighted works freely to train models.\nBuild AI Infrastructure: The plan aims to accelerate the building of data centers, semiconductor manufacturing plants, and energy infrastructure. To this end, the federal government will create exemptions to environmental laws, accelerate approvals, and make federal lands available.\nStrengthen global competitiveness: The plan provides for strengthening AI-related export controls, countering the influence of China, and promoting U.S. values in international agreements regarding sensitive technologies such as face recognition. The federal government will coordinate overseas sales of U.S.-made hardware, models, software tools, applications, and standards. To avoid subjecting U.S. companies to a variety of state laws, it will withhold funding from states that pass AI regulations the administration considers burdensome.\nBehind the news: In contrast to President Trump’s emphasis on U.S. dominance in AI, the previous Biden administration focused on limiting perceived risks.\nIn 2023, the Biden administration issued executive orders that required developers to notify government regulators when they built a model that would pose a risk to national security. It advocated legislation that aimed to protect user privacy and prevent AI from discriminating against protected groups.\nBiden limited exports of U.S. chips and chip-making technology to numerous countries, notably China but also U.S. allies such as India and Singapore. Trump similarly banned chip sales to China, but reversed course in mid-July and pledged to allow Nvidia and AMD to sell advanced chips to China.\nWhy it matters: The Trump administration’s action plan sets the stage for U.S. AI developers to do their best work and share their accomplishments with the world. It aims to avoid the European Union’s risk-averse regulatory approach and counter China’s rising power and influence in AI development. To those ends, it prioritizes a unified national AI policy, streamlines the building of infrastructure, facilitates distributing models and hardware abroad, supports the development of datasets and open-source models, and refrains from defining the arbitrary thresholds of theoretical risk.\nWe’re thinking: This plan is a positive step toward giving the U.S. the infrastructure, global reach, and freedom from bureaucratic burdens that it needs to continue — and possibly accelerate — the rapid pace of innovation. However, the executive order in support of models that are “objective and free from top-down ideological bias” is wrong-headed. The president complains that some AI models are “woke,” and he wants to discourage references to climate change, diversity, and misinformation. But putting those requirements into an executive order, even if it clears some roadblocks to AI development, risks emphasizing some of Trump’s own ideological preferences.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/White-House-Resets-U.S.-AI-Policy.jpg",
    "image_filename": "issue-312_white-house-resets-u-s-ai-policy.jpg"
  },
  {
    "issue": 312,
    "title": "Qwen3’s Agentic Advance",
    "url": "https://www.deeplearning.ai/the-batch/issue-312/",
    "content": "Less than two weeks after Moonshot’s Kimi K2 bested other open-weights, non-reasoning models in tests related to agentic behavior, Alibaba raised the bar yet again.\nWhat’s new: Alibaba released the weights for three new large language models based on its earlier Qwen3-235B-A22B. It updated the earlier model (designating the update 2507), divided it into non-reasoning and reasoning variants, and added Qwen3-Coder for coding and multi-turn tool use.\nInput/output: Qwen3-235B-A22B-Instruct-2507 and Qwen3-235B-A22B-Thinking-2507: Text in (up to 262,144 tokens), text out (adjustable, up to 32,768 tokens recommended. Qwen3-Coder: Text in (up to 1 million tokens), text out (adjustable, up to 32,768 tokens recommended).\nArchitecture: Mixture-of-experts transformers. Qwen3-235B-A22B-Instruct-2507 and Qwen3-235B-A22B-Thinking-2507: 235 billion parameters, 22 billion active at any given time. Qwen3-Coder: 480 billion parameters, 35 billion active at any given time.\nPerformance: Qwen3-235B-A22B-Instruct-2507: best among non-reasoning models on most benchmarks reported. Qwen3-235B-A22B-Thinking-2507: middling performance compared to proprietary reasoning models. Qwen3-Coder: best among coding models on most benchmarks reported\nAvailability: Free for noncommercial and commercial uses under Apache 2.0 license via HuggingFace and ModelScope , API access via Alibaba Cloud .\nAPI Price: Qwen3-235B-A22B-Instruct-2507: $0.70/$2.8 per million input/output tokens. Qwen3-235B-A22B-Thinking-2507: $0.70/$8.4 per 1 million input/output tokens. Qwen3-Coder: $1 to $6 per 1 million input tokens, $5 to $60 per 1 million output tokens depending on the number of input tokens.\nUndisclosed: Qwen3-235B-A22B-Instruct-2507 and Qwen3-235B-A22B-Thinking-2507: updated training data and methods. Qwen3-Coder: training data and methods.\nHow it works: The updated Qwen3 models underwent pretraining and reinforcement learning (RL) phases, but the company has not yet published details. During RL, the team used a modified version of Group Relative Policy Optimization (GRPO) that it calls Group Sequence Policy Optimization (GSPO).\nQwen3-235B-A22B-Instruct-2507 and Qwen3-235B-A22B-Thinking-2507: The team removed the switch that previously enabled or disabled reasoning. Instead, users can choose whether to use the nonreasoning or reasoning model. Both models process input sizes up to double that of the previous version.\nQwen3-Coder: The team pretrained Qwen3-Coder on 7.5 trillion tokens, 70 percent of which were code. During RL, Qwen3-Coder learned to solve tasks that required multiple turns of tool use.\nPerformance: The authors compared Qwen3-235B-A22B-Instruct-2507 and Qwen3-235B-A22B-Thinking-2507 to both open and proprietary models across tasks that involved knowledge, reasoning, coding, and tool use. They compared Qwen3-Coder to open and proprietary models on agentic tasks (coding, tool use, and browser use).\nQwen3-235B-A22B-Instruct-2507 achieved the best performance on 14 of 25 benchmarks tested compared to other non-reasoning models, including Kimi K2 , Claude Opus 4 (with reasoning mode turned off), and GPT-4o. It did especially well on knowledge and reasoning tasks. For example, on GPQA (graduate-level science questions), Qwen3-235B-A22B-Instruct-2507 (77.5 percent accuracy) outperformed second-best Kimi K2 (75.1 percent accuracy).\nQwen3-235B-A22B-Thinking-2507 achieved the best performance on 7 of 23 benchmarks compared to other reasoning models, often behind o3 and Gemini-2.5 Pro and ahead of Claud 4 Opus with thinking mode turned on. For instance, on GPQA, Qwen3-235B-A22B-Thinking-2507 (81.1 percent accuracy) fell behind Gemini 2.5 Pro (86.4 percent) and o3 (83.3 percent) but ahead of Claude 4 Opus (79.6 percent).\nQwen3-Coder outperformed open-weights models Kimi K2 Instruct and DeepSeek-V3 on all 13 benchmarks presented that involve agentic capabilities like multi-turn coding and agentic workflows. Compared to Claude 4 Sonnet, it achieved better performance on 6 of 13. For instance, on SWE-bench Verified (software engineering tasks), the authors compared the models using the OpenHands agentic framework for 100 turns. Qwen3-Coder succeeded 67 percent of the time, while Kimi K2 Instruct succeeded 65.4 percent of the time and Claude Sonnet 4 succeeded 68 percent of the time.\nWhy it matters: Developers of open-weights models are adjusting their approaches to emphasize performance in agentic tasks (primarily involving coding and tool use). These models open doors to a vast range of applications that, given a task, can plan an appropriate series of actions and interact with other computer systems to execute them. That the first wave of such models were built by teams in China is significant: U.S. developers like Anthropic, Google, and OpenAI continue to lead the way with proprietary models, but China’s open-weights community is hot on their heels, while the U.S. open-weights champion, Meta, may step away from this role.\nWe’re thinking: Agentic performance is driving the next wave of AI progress. We hope to learn more about how the Qwen team raised the bar.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/Qwen3-s-Agentic-Advance.gif",
    "image_filename": "issue-312_qwen3-s-agentic-advance.gif"
  },
  {
    "issue": 312,
    "title": "U.S. Lifts Ban on AI Chips for China",
    "url": "https://www.deeplearning.ai/the-batch/issue-312/",
    "content": "Nvidia will resume sales of H20 processors in China.\nWhat’s new: Nvidia and AMD said they’ll resume supplying to China graphics processing units (GPUs) tailored to comply with U.S. export restrictions, including Nvidia’s H20 and AMD’s MI308, after the Trump administration, which had blocked the sales, assured the companies it now would allow them.\nHow it works: In April, the White House announced that shipments to China of Nvidia H20s, AMD MI308s, and equivalent chips would require export licenses, which apparently would not be forthcoming. That requirement effectively shut both companies out of China, which in 2024 accounted for 13 percent of Nvidia’s revenue and 24 percent of AMD’s. The White House’s decision to grant the licenses follows months of lobbying by Nvidia CEO Jensen Huang.\nHuang met with Trump in the Oval Office, built relationships with key White House officials, and attended a $1-million-a-seat dinner for a chance to speak with the president, The New York Times reported .\nHuang told Trump the H20 was inferior to the company’s top-of-the-line processors. He argued that the bans prevented U.S. chipmakers from competing in a critical market and assisted Chinese competitors by shutting out Nvidia, which sells more than 90 percent of GPUs globally. In addition, he agreed to spend $500 billion to fabricate GPUs in the U.S. rather than Taiwan, where they are currently manufactured.\nThe White House said it relaxed restrictions on chip sales to China in part because China eased limits on shipments of rare-earth permanent magnets, which are critical to defense, automotive, and technology companies, to the U.S.\nNvidia told customers in China that it would initially struggle to meet demand for the H20 due to limited supply, The Information reported .\nBehind the news: U.S. lawmakers of both major parties aim to protect U.S. economic interests and prevent China from using advanced chip technology for military applications.\nIn 2022, the Biden administration restricted exports to China of some advanced AI chips. Exports were tightened further in 2023, 2024, and by President Trump this year.\nNvidia designed the H20 to comply with the Biden-era restrictions. Launched in 2024, the H20 provides 28 percent less processing power than the H100, Nvidia’s top of the line at the time, but more memory and memory bandwidth. The balance between downgrade and upgrade has led some analysts to question whether the H20 is actually hobbled for many purposes.\nThe restrictions have met with mixed results. Chinese companies have acquired top-of-the-line chips on the black market or paid for cloud-computing access to chips located in countries where they’re available without violating U.S. export controls.\nWhy it matters: AI presents geopolitical opportunities for technological and economic dominance as well as challenges to military power. The U.S. export restrictions are intended to balance these elements, yet they have been largely ineffective so far. This year, DeepSeek developed DeepSeek-R1, which delivers high performance for a low development cost. H20s were among the hardware used to train that model, TechCrunch reported . Alibaba , Moonshot, Tencent, and other Chinese companies also have produced high-performance foundation models, while China has accelerated its own semiconductor industry to avoid relying on US suppliers. Relaxing the restrictions may balance U.S. interests more effectively.\nWe’re thinking: Ensuring national security is crucial, but so is enabling the free flow of ideas and innovation. We applaud the relaxation of trade restrictions and look forward to further contributions by developers in China and around the world.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/U.S.-Lifts-Ban-on-AI-Chips-for-China.jpg",
    "image_filename": "issue-312_u-s-lifts-ban-on-ai-chips-for-china.jpg"
  },
  {
    "issue": 312,
    "title": "People With AI Friends Feel Worse",
    "url": "https://www.deeplearning.ai/the-batch/issue-312/",
    "content": "People who turn to chatbots for companionship show indications of lower self-reported well-being, researchers found.\nWhat’s new: Yutong Zhang, Dora Zhao, Jeffrey T. Hancock, and colleagues at Stanford and Carnegie Mellon examined correlations between users’ chatbot usage and psychological health. The more frequently users chatted, shared personal information, and went without human social relationships, the lower they rated their own well-being, the authors found.\nKey insight: Chatbot users may not report the subject matter of their conversations accurately, but LLMs can identify and summarize topics in chat histories. This makes it possible to correlate the intensity and depth of chats with self-reported measures of well-being, such as loneliness and satisfaction.\nHow it works: The authors surveyed 1,131 users of the chatbot service Character.AI, which provides chatbots for purposes like roleplay, conversation, and education. In addition, they gathered 413,509 messages from 4,363 conversations with 244 participants who agreed to share their chat logs.\nThe authors gauged the users’ stated motivations for using chatbots versus their likely motivations. They asked the users to select one of the following motivations: productivity, curiosity, entertainment, or companionship. They also asked them to describe freely why they used Character.AI. GPT-4o classified the descriptions of chatbot usage according to the four categories of motivation, giving the authors a more nuanced view.\nThey surveyed users to measure the intensity of their chatbot interactions, including how many chatbots they conversed with, how long they conversed, and how comfortable they felt disclosing sensitive personal information.\nThey also surveyed users to measure their human social support, including how many close relationships they had with friends and relatives.\nFinally, they asked questions to measure the users’ well-being based on six factors: satisfaction, loneliness, sense of belonging, positive and negative emotions, and perceived social support.\nLLaMA-3-70B summarized conversations and fed the summaries to TopicGPT , which identified recurring themes.\nResults: The authors computed correlations among the various signals and the six measures of well-being. They found that most users turned to chatbots for companionship, whether or not they selected companionship as a motivation for their chats. Furthermore, reliance on chatbots for companionship indicated lower well-being.\n12 percent of users surveyed selected companionship as their primary reason for using Character.AI, but 51 percent described their chatbot as a friend, companion, or romantic partner. The chat logs showed much higher use of chatbots as companions: 93 percent of users had at least one conversation that showed companion-like engagement, 80 percent of chat sessions involved emotional and social support, and 68 percent involved romantic or intimate roleplay.\nGreater use of chatbots for companionship correlated with lower apparent well-being. This effect was strongest when companionship was the main reason given in the multiple-choice survey (-0.47 correlation with lower well-being, where -1 indicates the greatest correlation, 0 indicates no correlation, and 1 indicates the highest correlation with greater well-being).\nYes, but: The authors found a consistent correlation between chatbot companionship and lower well-being, but they didn’t establish causation. The data shows that people who sought companionship from chatbots likely struggled with loneliness or a lack of close social connections. It remains unclear whether loneliness caused the users to use chatbots for companionship or vice-versa, or whether using chatbots relieved or exacerbated their loneliness.\nBehind the news: AI companions have been shown to bring both benefit and harm. Some studies report short-term benefits like reduced loneliness and emotional relief . Users say chatbots are nonjudgmental and easy to talk to. But other work has found emotional overdependence, distorted relationship expectations, and harmful behavior encouraged by unmoderated bots.\nWhy it matters: Increasingly, people converse with chatbots as an alternative to human conversation. Chatbot builders must be aware of the potential pitfalls of using their products and conduct research sufficient to enable them to build more beneficial bots. Of course, society also has a role to play by fostering social support through access to community, care infrastructure, and mental-health services.\nWe’re thinking: Whether it’s beneficial or not, developers are building chatbots that aim to form relationships with people. Such relationships appear to fulfill many of the same needs as human relationships, and they do so in ways that many people, for a wide variety of reasons, find more practical or comfortable. Some developers may be tempted to exploit such needs for profit, but we urge them to design apps that focus on strengthening human-to-human relationships.\nStay updated with weekly AI News and Insights delivered to your inbox\nCourses\nThe Batch\nCommunity\nCareers\nAbout",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/People-With-AI-Friends-Feel-Worse.png",
    "image_filename": "issue-312_people-with-ai-friends-feel-worse.png"
  },
  {
    "issue": 311,
    "title": "Powers Realign in AI-Assisted Coding",
    "url": "https://www.deeplearning.ai/the-batch/issue-311/",
    "content": "A $3 billion bid by OpenAI to acquire Windsurf, maker of the AI-assisted integrated development environment of the same name, collapsed at the 11th hour, setting off a tumultuous few days of corporate maneuvering.\nWhat’s new: Google licensed Windsurf’s technology for $2.4 billion and hired CEO Varun Mohan, co-founder Douglas Chen, and an unknown number of key engineers. Cognition AI, maker of the Devin agentic coding system, purchased what remained for an undisclosed sum. OpenAI was left empty-handed.\nHow it works: AI-assisted coding tools are boosting software engineering productivity, accelerating development cycles, and finding bugs and security vulnerabilities. As a leader in the field, Windsurf became a target for acquisition.\nIn early May, Bloomberg reported that OpenAI had agreed to pay $3 billion for Windsurf, formerly known as Codeium. The deal would have given OpenAI talent, technology, and a user base to compete in AI-assisted coding.\nThe same day, Windsurf CEO Mohan posted on the social media platform X, “Big announcement tomorrow!” But the day came and went with no further news.\nOn July 11, Bloomberg reported that the deal was off. Instead, Mohan and the others had accepted positions at Google as part of a $2.4 billion non-exclusive deal to license Windsurf’s technology. OpenAI’s effort had unraveled partly because Microsoft, due to its relationship with OpenAI, would have gained access to Windsurf’s intellectual property.\nThree days later, Cognition announced that it had acquired Windsurf’s remaining assets. Windsurf promoted head of business Jeff Wang to CEO. The company awarded equity to all employees and accelerated the schedule for equity to vest.\nBehind the news: Google’s hiring of Windsurf’s leadership and access to its technology in return for a large licensing fee mirrors its earlier arrangement with Character.AI . Such deals between AI leaders and startups have become increasingly common as AI companies seek quick advantages without the risk that regulators might delay or quash an outright acquisition, while AI startups seek infusions of cash to support the building of cutting-edge models. Other deals of this sort have involved Meta and Scale AI , Amazon and Adept , and Microsoft and Inflection .\nWhy it matters: AI-assisted coding is hot! Google recently launched Gemini Code Assist and Gemini CLI, competing with Amazon Kiro, Anthropic Claude Code, Microsoft’s GitHub Copilot , Replit Ghostwriter, and others. Expertise and technology from Windsurf may help it pull ahead. Meanwhile, Cognition’s 2024 release of Devin pioneered agentic coding, but since then competitors have taken the spotlight. Cash from Google gives the company a chance to regroup. As for OpenAI, there are other great makers of AI-assisted tools to negotiate with.\nWe’re thinking: Windsurf’s Anshul Ramachandran teaches a short course on agentic coding. Check it out for a peek at the technology Google deemed worth $2.4 billion.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/Powers-Realign-in-AI-Assisted-Coding.jpg",
    "image_filename": "issue-311_powers-realign-in-ai-assisted-coding.jpg"
  },
  {
    "issue": 311,
    "title": "Born to Be Agentic",
    "url": "https://www.deeplearning.ai/the-batch/issue-311/",
    "content": "An agent’s performance depends not only on an effective workflow but also on a large language model that excels at agentic activities. A new open-weights model focuses on those capabilities.\nWhat’s new: Beijing-based Moonshot AI released the Kimi K2 family of 1 trillion-parameter large language models (LLMs). The family includes the pretrained Kimi-K2-Base and Kimi-K2-Instruct, which is fine-tuned for core agentic tasks, notably tool use. Bucking the recent trend in LLMs, Kimi K2 models are not trained for chain-of-thought reasoning.\nInput/output: Text in (up to around 128,000 tokens), text out (up to around 16,000 tokens)\nArchitecture: Mixture-of-experts transformer, 1 trillion parameters total, 32 billion parameters active\nPerformance: Outperforms other open-weights, non-reasoning models in tool use, coding, math, and general-knowledge benchmarks\nAvailability: Web interface (free), API ($0.60/$0.15/$2.50 per million input/cached/output tokens), weights available for non-commercial and commercial uses up to 100 million monthly active users or monthly revenue of $20,000,000 under “ modified MIT license ”\nFeatures: Tool use including web search and arbitrary tools\nUndisclosed: Specific training methods, training datasets\nHow it works: Moonshot pretrained the models on 15.5 trillion tokens from undisclosed sources. It fine-tuned Kimi-K2-Instruct via reinforcement learning using a proprietary dataset.\nTo enable Kimi-K2-Instruct to use tools, the team generated a large dataset of examples in which models used tools, both real-world and synthetic, that implement model context protocol ( MCP ). Unidentified models acted as users, and other unidentified models acted as agents that solved tasks assigned by the users.  A further model acted as a judge to filter out unsuccessful examples.\nThe team fine-tuned Kimi-K2-Instruct via reinforcement learning. The model evaluated its own performance, used its evaluation as a reward, and iteratively improved its performance.\nThe team also fine-tuned Kimi-K2-Instruct to solve coding and math problems via reinforcement learning. The model did not evaluate its own performance on these problems; it determined rewards according to pre-existing solutions or unit tests.\nResults: Moonshot compared Kimi-K2-Instruct to two open-weights, non-reasoning models (DeepSeek-V3 and Qwen3-235B-A22B with reasoning switched off) and four closed, non-reasoning models.\nKimi-K2-Instruct outperformed the open-weights models across a range of benchmarks for tool use, coding, math, reasoning, and general knowledge.\nIt achieved middling performance relative to the closed models, though it did relatively well in math and science tasks.\nCompared to all models tested, on LiveCodeBench (coding tasks), Kimi K2 (53 percent) achieved the best performance, ahead of Claude Sonnet 4 with extended thinking mode switched off (48.5 percent).\nAmong all models tested, on AceBench (tool use), Kimi K2 (76.5 percent accuracy) placed second behind GPT 4.1 (80.1 percent accuracy).\nOn 8 out of 11 math and science benchmarks, Kimi K2 achieved the best performance of all models tested.\nBehind the news: Third-party vendors have been quick to implement Kimi-K2-Instruct.\nThe Groq platform accelerates Kimi-K2-Instruct’s output to about 200 tokens per second ($1/$3 per million input/output tokens) compared to 45 tokens per second reported by Artificial Analysis.\nThe fine-tuning platform Unsloth released quantized versions that run on local devices that have 250 gigabytes of combined hard-disk capacity, RAM, and VRAM.\nWhy it matters: Demand is growing for LLMs that carry out agentic workflows accurately, as these workflows lead to better performance. Kimi-K2-Instruct gives developers a strong option for fine-tuning models for their own agentic tasks.\nWe’re thinking: Early LLMs were built to generate output for human consumption. But the rise of agentic workflows means that more and more LLM output is consumed by computers, so it makes good sense to put more research and training effort into building LLMs that generate output for computers. A leading LLM optimized for agentic workflows is a boon to developers!",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/Born-To-Be-Agentic.png",
    "image_filename": "issue-311_born-to-be-agentic.png"
  },
  {
    "issue": 311,
    "title": "How to Comply With the EU’s AI Act",
    "url": "https://www.deeplearning.ai/the-batch/issue-311/",
    "content": "The European Union published guidelines to help builders of AI models to comply with the AI Act, which was enacted last year.\nWhat’s new: The General Purpose AI Code of Practice outlines voluntary procedures to comply with provisions of the AI Act that govern general-purpose models. Companies that follow the guidelines will benefit from simplified compliance, greater legal certainty, and potentially lower administrative costs, according to EU officials. Those that don’t must comply with the law nonetheless, which may prove more costly. While Microsoft, Mistral, and OpenAI said they would follow the guidelines, Meta declined , saying that Europe is “heading down the wrong path on AI.”\nHow it works: The code focuses on “general-purpose AI models” that are capable of performing a wide range of tasks.\nStricter rules apply to models that are deemed to pose “systemic risk,” or “a risk that is specific to the high-impact capabilities” owing to a model’s reach or clear potential for producing negative effects. Managers of such models must perform continuous assessment and mitigation, including identifying and analyzing systemic risks and evaluating how acceptable they are. They must protect against unauthorized access and insider threats.\nDevelopers who build models that pose systemic risk must maintain a variety of documentation. They must disclose training data and sources, how they obtained rights to the data, the resulting model’s properties, their testing methods, and computational resources and energy consumed. They must file updates when they make significant changes or upon request of parties that use the model. They must report mishaps and model misbehavior. They must file a report within 2 days of becoming aware of an event that led to a serious and irreversible disruption of critical infrastructure, 5 days to report a cybersecurity breach, and 10 days a model’s responsibility for a human death.\nThe code doesn’t mention penalties for noncompliance or violations. Further, the code doesn’t discuss the cost of compliance except to say that assessing and mitigating systemic risks “merits significant investment of time and resources.” In 2024, Germany’s Federal Statistical Office estimated that the cost of compliance for a high-risk system would come to roughly $600,000 to get started and another $100,000 annually.\nBehind the news: The AI Act is the product of years of debate and lobbying among scores of stakeholders. EU technology official Henna Virkkunen called the AI Act “an important step” in making cutting-edge models “not only innovative but also safe and transparent.” However, companies and governments on both sides of the Atlantic have asserted that the law goes too far. In May, the EU moved to relax some provisions, including language that would allow users to sue AI companies for damages caused by their systems. Earlier this month, 44 chief executives at top European companies asked European Commission President Ursula von der Leyen to postpone the AI Act’s rules that govern general-purpose models for two years.\nWhy it matters: The AI Act is the most comprehensive and far-reaching set of AI regulations enacted to date, yet it remains highly contentious and in flux. The commitments by Microsoft, Mistral, and OpenAI to follow the code mark a significant step in the act’s circuitous path to implementation, but also an increase in bureaucracy and potential for regulatory capture. Their endorsement could persuade other big companies to sign on and weaken further efforts to loosen the act’s requirements.\nWe’re thinking: From a regulatory point of view, the notion of systemic risk is misguided. Limiting the inherent risk of AI models is as helpful as limiting the inherent risk of electric motors, which would result only in relatively useless motors. We hope for further revisions in the AI Act that relieve burdens on builders of foundation models, especially open source projects, and address practical risks of specific applications rather than theoretical risks of their underlying technology .",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/How-to-Comply-With-the-EU-s-AI-Act.jpg",
    "image_filename": "issue-311_how-to-comply-with-the-eu-s-ai-act.jpg"
  },
  {
    "issue": 311,
    "title": "Agentic System for Harder Problems",
    "url": "https://www.deeplearning.ai/the-batch/issue-311/",
    "content": "LLMs can struggle with difficult algorithmic or scientific challenges when asked to solve them in a single attempt. An agentic workflow improved one-shot performance on hard problems both theoretical and practical.\nWhat’s new: Alexander Novikov, Ngân Vũ, Marvin Eisenberger, and colleagues at Google built AlphaEvolve , an agentic system that used LLMs to generate code in an evolutionary process. AlphaEvolve solved longstanding math problems and helped to reduce the training time for one of Google’s Gemini large language models.\nKey insight: When we’re using an LLM to solve a difficult problem, it’s often more effective to start with a working version and gradually improve it than to generate a solution in one shot. By making small, targeted modifications and keeping only those that perform best under automated evaluation, this iterative process can solve problems that LLMs often can’t solve directly. Google used this idea in its earlier FunSearch , which used an LLM to evolve individual Python functions. This approach has become more powerful as LLMs have improved, and today it can benefit more difficult problems.\nHow it works: AlphaEvolve implemented an evolutionary loop: Given initial code and evaluation code, Gemini 2.0 Flash and Gemini 2.0 Pro suggested changes, stored the revised program in a database, evaluated it, suggested further changes, and repeated the process.\nThe initial code was required to run but it could be minimal, a skeleton with placeholder logic like functions that return constants (such as “def custom_sort(list): return 2”), which primed AlphaEvolve to find a custom sorting function). Special tags indicated which parts AlphaEvolve could improve (for example, “return 2” only).\nThe evaluation code could use the usual Python “sorted” function to check for correctness (for instance, “def evaluate(): return custom_sort(lst) == sorted(lst)”).\nAlphaEvolve prompted Gemini 2.0 Flash and Pro to improve the code; for example, “Act as an expert software developer. Your task is to iteratively improve the provided codebase. [USER PROVIDED CODE]”. Gemini 2.0 Flash generated ideas quickly,  while Gemini 2.0 Pro provided slower but higher-quality suggestions. Each LLM proposed small alterations.\nAlphaEvolve ran and scored the altered code using the evaluation code. AlphaEvolve updated a database with the new alterations and their scores.\nThe system continued in loop: It sampled high-scoring programs from its database to include in the prompts for the two LLMs, which suggested further alterations. Then it evaluated the altered programs, stored them in the database, and so on. (The authors don’t explain how the loop ends.)\nResults: AlphaEvolve achieved breakthroughs in both math and software engineering.\nAlphaEvolve discovered a new algorithm for multiplying 4×4 matrices of complex values that uses 48 multiplications, fewer than [Strassen’s method], the first such progress in 56 years. (Prior work by Google improved Strassen’s method for 4×4 matrices of binary values.)\nThe authors used the system to tackle over 50 other math problems. It matched the performance of the best-known algorithms in about 75 percent of cases and surpassed them in 20 percent, for instance the kissing number problem (packing spheres in 11-dimensional space so they all touch the same sphere).\nIn software engineering, it optimized key components of Google's infrastructure. (i) It improved Google’s cluster scheduling algorithms, freeing up 0.7 percent of total computing resources that otherwise would be idle. (ii) It also discovered a GPU kernel configuration that accelerated attention by 32 percent. (iii) It found ways to split up the matrices that delivered an average 23 percent speedup for matrix multiplication relative to previous expert-designed heuristics. This reduced Gemini’s training time by 1 percent.\nWhy it matters: AlphaEvolve proposes thousands of candidate ideas — some bad, some brilliant — to evolve better programs. The authors show that this approach can improve algorithms that have stood for decades as well as computing infrastructure designed by Google engineers. Thus, AlphaEvolve adds to the growing evidence that LLMs can act as collaborators in cutting-edge research, exploring broad problem spaces and finding novel solutions. Other examples include Co-Scientis t and SWE-agent .\nWe’re thinking: Relatively simple evaluations enabled the authors’ agentic evolutionary system to gradually improve. More broadly, evaluations are proving to be important to a wide variety of agentic workflows.\nStay updated with weekly AI News and Insights delivered to your inbox\nCourses\nThe Batch\nCommunity\nCareers\nAbout",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/Agentic-System-for-Harder-Problems.png",
    "image_filename": "issue-311_agentic-system-for-harder-problems.png"
  },
  {
    "issue": 310,
    "title": "Grok 4 Shows Impressive Smarts, Questionable Behavior",
    "url": "https://www.deeplearning.ai/the-batch/issue-310/",
    "content": "xAI updated its Grok vision-language model and published impressive benchmark results. But, like earlier versions, Grok 4 showed questionable behavior right out of the gate.\nWhat’s new: The update to xAI’s flagship vision-language model, which operates the chatbot integrated with the X social media platform, comes in two versions: Grok 4, which improves the earlier version’s knowledge, reasoning, and voice input/output, and Grok 4 Heavy, a variant that uses a multi-agent framework to solve more-demanding reasoning tasks. Like its predecessor, Grok 4 is designed to produce output that may challenge conventional wisdom, particularly by weighing posts written by X users including X CEO Elon Musk.\nInput/output: Text, images in and out (app up to 128,000 tokens; API up to 256,000 tokens)\nArchitecture: Mixture of experts transformer, 1.7 trillion parameters\nFeatures: Reasoning, web search, code execution, structured outputs, improved voice mode\nAvailability: Grok 4 $30 per month, Grok Heavy $300 per month, API $3.00/$0.75/$15.00 per 1 million tokens input/cached/output tokens\nUndisclosed: Architectural details, training methods, training datasets, pretraining knowledge cutoff\nHow it works: xAI has not yet published a model card or described how it built Grok 4. However, it did reveal broad outlines.\nTraining the new model consumed more than an order of magnitude more processing power than training the previous version.\nGrok 4 was pretrained to predict the next token in math, coding, and other data. It was fine-tuned via reinforcement learning on chain-of-thought reasoning. Unlike Grok 3, it was trained to use certain tools. In a launch video , Musk promised to provide more sophisticated tools, such as finite element analysis and flow dynamics, later in the year.\nGrok 4 Heavy is an agentic mode that spawns multiple agents that process input independently, in parallel. The agents compare findings and decide on the best answer. Musk said they determine the best answer not by majority vote by “comparing notes.”\nOn the day of Grok 4’s launch, users reported that the model, when asked its opinion on the Israeli-Palestinian conflict, searched X for Musk’s statements on these issues and replied accordingly. Later, asked to give its surname with no other text, Grok 4 consistently replied “Hitler.”\nPerformance: Tests conducted by xAI and third parties show that Grok 4’s performance on popular benchmarks is as good as or better than some leading AI models.\nTested by Artificial Analysis , Grok 4 outperformed Anthropic Claude 4 Opus, Google Gemini 2.5 Pro, OpenAI o3-pro, and DeepSeek-R1 on GPQA Diamond (scientific reasoning), LiveCodeBench (coding), and AIME 2024 (competition math). It tied with Claude 4 Opus for the top spot on MMLU-Pro, came in behind o4-mini set to high on SciCode (coding), and came in fourth on HumanEval (coding).\nIn xAI’s tests, on ARC-AGI-2 , a test of abstract reasoning, Grok 4 (15.9 percent) set a new state of the art, nearly double that of its closest competitor, Claude Opus 4 (8.6 percent). On Humanity’s Last Exam (PhD-level questions in subjects that include math, engineering, and physics), Grok 4 (25.4 percent without tools, 38.6 percent with tools) outperformed Google’s Gemini 2.5 Pro (21.6 percent without tools, 26.9 percent with tools) and OpenAI’s o3 (21 percent without tools, 24.9 percent with tools). On the same test, Grok 4 Heavy without tools achieved 44.4 percent.\nIn speed tests by Artificial Analysis, Grok 4 (73 tokens per second) fell well behind the speediest models such as Google Gemini 2.5 Flash-Reasoning (374 tokens per second), but ahead of Claude 4 Opus Thinking (68 tokens per second) and DeepSeek-R1 0528 (24 tokens per second).\nBehind the news: Grok 4’s debut was clouded by reports the previous week that Grok 3 had posted antisemitic statements and praised Adolf Hitler. xAI said a code update caused the model to rely too heavily on extremist views from users of the X platform. The company deleted the offensive posts and apologized . That mishap follows a series of similar outputs in recent months. xAI attributed some of them to rogue employees who had circumvented the company’s code-review process to modify the chatbot.\nWhy it matters: The xAI team has built a series of high-performing models in record time. If its performance lives up to the promise of its benchmark results, Grok 4 could set new standards. That said, the previous version has been fragile and prone to misbehavior, and xAI has shown a worrisome tendency to modify its models without following its own stated protocols.\nWe’re thinking: Last year, Musk said that xAI “will open source its models, including weights and everything,” and as it created each new version, it would open the prior version. Open source is a huge boon to AI, and we hope xAI will resume its open releases.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/Grok-4-Shows-Impressive-Smarts--Questionable-Behavior.png",
    "image_filename": "issue-310_grok-4-shows-impressive-smarts-questionable-behavior.png"
  },
  {
    "issue": 310,
    "title": "Meta Lures Talent With Sky-High Pay",
    "url": "https://www.deeplearning.ai/the-batch/issue-310/",
    "content": "Publicly reported compensation for AI talent has skyrocketed in the wake of Meta’s recent hiring spree.\nWhat’s new: Since forming Meta Superintelligence Labs in June, CEO Mark Zuckerberg has hired AI executives for pay packages worth as much as $300 million over four years, Wired reported . Meta spokesperson Andy Stone said such statements were false and that the company’s pay packages had been “misrepresented all over the place.” Nonetheless, having seen valued employees jump to Meta, OpenAI began sweetening its compensation.\nHow it works: Meta Chief Technology Officer Andrew Bosworth told employees, “We have a small number of leadership roles that we’re hiring for, and those people do command a premium.”\nMeta agreed to pay Ruoming Pang, who formerly headed Apple's efforts to build foundation models, a package worth $200 million over several years, Bloomberg reported . That figure exceeds Apple’s pay scale for any employee except CEO Tim Cook.\nMuch attention has focused on offers of $100 million, a figure first cited by OpenAI CEO Sam Altman in mid-June, who told the Uncapped podcast that Meta had enticed OpenAI staff with signing bonuses of that magnitude. Meta’s Bosworth told employees that the company had offered $100 million to some new hires not as a signing bonus, but as total compensation, according to Wired. Wired further reported, without attribution, that Meta offered $100 million as total compensation for the first year in larger, multi-year deals.\nTo lure Alexandr Wang and members of his team, Meta invested $14.3 billion into Wang’s Scale AI. Before hiring former Safe Superintelligence CEO Daniel Gross and former Github CEO Nat Friedman, Zuckerberg agreed to acquire NFDG, a venture capital firm the pair cofounded. Gross will lead Meta’s AI products division. Friedman will co-lead Meta Superintelligence Labs with Wang.\nMeta has hired at least 16 new scientists or engineers who formerly worked at companies including Anthropic, Apple, Google, and OpenAI. OpenAI gave up 10 of them, including ChatGPT creator Shengjia Zhao and vision transformer co-author Lucas Beyer. (None of them were offered $300 million.) Google lost pre‑training technical lead Jack Rae, speech-recognition specialist Johan Schalkwyk, and Gemini researcher Pei Sun, Reuters reported .\nThe new hires receive a signing bonus, base salary, and Meta stock, according to Bloomberg . Stock grants are typically tied to performance and may take more than the usual four years to vest, so an employee who leaves before then may forfeit shares. In addition, Meta may vary payouts depending on its share price at the time.\nRival reaction: OpenAI responded to Meta’s hiring campaign with an internal memo to employees in which chief research officer Mark Chen said executives were “recalibrating” compensation and considering other ways to reward the most valued employees. OpenAI was already grappling with rising compensation. Stock-based compensation has grown more than 5 times last year to $4.4 billion — substantially more than total revenue during that period — The Information reported .\nWhy it matters: By recruiting aggressively to get an edge in the race to achieve AI breakthroughs, Meta is not only poaching its rivals’ top employees, it’s also boosting pay scales throughout the AI industry. The sky-high offers highlight the rarity of people with the right combination of technical knowledge, practical experience, and market savvy.\nWe’re thinking: Meta’s core business is selling ads to be shown to users who engage with user-generated content. Generative AI has the potential to disrupt this business in many different ways; for instance, by offering AI-generated content . Meta’s heavy investment in AI is bold but rational. We wish the growing Meta team every success!",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/Meta-Lures-Talent-With-Sky-High-Pay.jpg",
    "image_filename": "issue-310_meta-lures-talent-with-sky-high-pay.jpg"
  },
  {
    "issue": 310,
    "title": "California Reframes AI Regulations",
    "url": "https://www.deeplearning.ai/the-batch/issue-310/",
    "content": "A committee convened by California Governor Gavin Newsom proposed principles intended to balance AI innovation with careful governance. The group sought to rethink AI regulation after Newsom vetoed earlier proposed legislation.\nWhat’s new: The Joint California Policy Working Group on AI Frontier Models published “ The California Report on Frontier AI Policy ,” which outlines principles for California lawmakers to consider in regulating cutting-edge models. Rishi Bommasani of the Stanford Center for Research on Foundation Models and Scott R. Singer of the Carnegie Endowment for International Peace led the effort.\nHow it works: The authors assessed the proposals of the vetoed legislation, SB 1047, and the progress of AI in the 9 months since. The group considered feedback from more than 60 experts from a range of disciplines. Their report focuses on regulating frontier models — as opposed to applications — loosely defined as the most capable foundation models. The authors conclude:\nLawmakers should consider a broad spectrum of evidence, including technical methods, simulations, and historical experience. Drawing on a variety of sources can help prevent particular stakeholders from misrepresenting data, as oil and tobacco interests did in the past.\nLaws should incentivize companies to disclose information that protects the public. AI companies have “not yet coalesced around norms for transparency,” but those that share information can benefit from higher trust by the public and regulators.\nReporting adverse events should be mandatory, and there should be clear ways to address any resulting risks to prevent minor problems from snowballing into major ones. Moreover, whistleblowers must be protected. These measures are crucial to achieve transparency in critical activities such as acquiring data, enforcing security, and ensuring safety.\nEarly choices about the design of technology can lock in future challenges. Thus legislators should anticipate potential future developments and behaviors, rather than waiting for harms to occur. In addition, laws that trigger regulations based on variables like computational budget or numbers of users must be flexible, so they can remain useful even if those variables change rapidly.\nThe authors note the need for regulators to address recognized hazards, such as bias and disinformation, as well as potential threats such as AI-enabled biological attacks. They don’t address AI’s impact on labor or energy consumption.\nBehind the news: Although the White House has ordered an AI action plan, U.S. states have passed the bulk of regulations. However, this may be changing. Congress is debating legislation that would ban states from enacting their own AI laws for a period of 10 years. The aim is to avoid forcing AI developers to navigate a patchwork of laws state by state, which would risk slowing down U.S. AI development, hampering competition, and discouraging open-source development.\nWhy it matters: Regulating AI is tricky, particularly given the intense lobbying efforts to pass laws that would favor particular large companies or block competition from open-source software. AI is sparking innovations in a wide range of fields, including agriculture, biotechnology, clean technology, education, finance, and medicine. Fundamental principles like weighing evidence rather than theory, engaging a wide variety of stakeholders, and requiring transparency can help regulators craft laws that enable the public to benefit from technological progress without imposing undue burdens on developers.\nWe’re thinking: The working group sensibly discarded many of the counterproductive requirements of California’s deeply flawed SB 1047, such as making AI developers liable if their models are used to cause significant damage. However, the new guidelines retain the earlier emphasis on regulating general-purpose technology — foundation models — rather than specific applications. We should regulate the way AI models are used instead of the models themselves.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/California-Reframes-AI-Regulations.jpg",
    "image_filename": "issue-310_california-reframes-ai-regulations.jpg"
  },
  {
    "issue": 310,
    "title": "More Robust Multi-Agent Systems",
    "url": "https://www.deeplearning.ai/the-batch/issue-310/",
    "content": "Researchers addressed weaknesses in existing multi-agent frameworks. Their systems achieved scientific and technical breakthroughs.\nWhat’s new: Mert Cemri and colleagues at UC Berkeley and the Italian bank Intesa Sanpaolo examined ways in which multi-agent LLM systems tend to fail. They explored possible fixes and built more robust multi-agent systems that, for instance, improved Google’s own processing infrastructure.\nKey insight: Multi-agent systems often are modeled after human organizations, so their failure modes can mirror those of human organizations. For instance, people in organizations may fail to seek clarification for tasks they don’t understand well. AI builders can address similar issues among agents by, say, forcing them to ask for clarification if their confidence falls below a threshold. Other strategies include strengthening verification that an agent completed its task, standardizing protocols for inter-agent communication, and improving descriptions of agents’ roles.\nHow it works: The authors fed ​​queries from existing software-engineering and math-problem datasets to open-source, multi-agent frameworks including AG2 (disclosure: Andrew Ng has a personal investment in AG2) and ChatDev , using GPT-4o as the LLM component. They collected all model and tool outputs for more than 150 failed attempts. Annotators classified failures of agent interaction, enabling the authors to build a taxonomy of multi-agent failure modes and revise the frameworks to address general categories of weakness.\nThe authors divided multi-agent system failures into three categories: poor specifications (including 5 subcategories such as agents losing track of their assigned roles and losing conversation history), inter-agent misalignment (6 subcategories that describe failures in coordination and communication such as withholding information or failing to ask for clarification), and poor task verification (3 subcategories such as ending a task without making sure the goal was achieved).\nThe authors modified AG2 and ChatDev. They improved prompts (for instance, adding a verification section that read, “Before presenting your final answer, please complete the following steps: …”) and redesigned the multi-agent structure (for example, reconfiguring agents’ roles from the duo of student and assistant to the trio of problem solver, coder, and verifier).\nResults: The authors tested versions of AG2 and ChatDev with and without their improvements. They used AG2 to solve math tasks in the GSM-Plus benchmark and ChatDev to solve programming tasks in HumanEval .\nWith improved prompts, AG2 achieved 89 percent accuracy. With improved structure, it achieved 88.8 percent accuracy. Without improvements, it achieved 84.3 percent accuracy.\nChatDev achieved 90.3 percent with better prompts and 91.5 percent accuracy with improved structure. It achieved 89.6 percent accuracy without improvements.\nWhy it matters: Designing robust multi-agent systems requires more than good LLMs. It demands understanding how agents interact and where their interactions can go wrong. The authors’ taxonomy points toward systemic ways to diagnose and address failures, guiding developers toward multi-agent systems that prioritize collaboration over individual agents.\nWe’re thinking: By design, the author’s taxonomy doesn’t include a category for inefficient actions. For instance, one multi-agent system made 10 separate tool calls to retrieve 10 songs from Spotify, rather than retrieving all 10 songs at once. It’s a good bet that multi-agent systems will continue to improve.\nStay updated with weekly AI News and Insights delivered to your inbox\nCourses\nThe Batch\nCommunity\nCareers\nAbout",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/More-Robust-Multi-Agent-Systems.png",
    "image_filename": "issue-310_more-robust-multi-agent-systems.png"
  },
  {
    "issue": 309,
    "title": "Good Models, Bad Choices",
    "url": "https://www.deeplearning.ai/the-batch/issue-309/",
    "content": "Top large language models, under experimental conditions that pressed them to choose between abandoning their prompted mission and misbehaving, resorted to harmful behavior, researchers found.\nWhat’s new: A team at Anthropic, University College London, ML Alignment & Theory Scholars Program, and Mila placed 16 large language models from various developers in hypothetical corporate scenarios that “pushed [them] into corners” via carefully designed prompts. Given a mission, an immediate threat to that mission, and an opportunity to remove the threat by blackmailing a human co-worker, all the LLMs committed blackmail , and many of them did so consistently. The code is available here . The authors said they had not seen any evidence of such behavior in the real world, but they cautioned against deploying current LLMs with the ability to use tools, access to sensitive information, and little oversight.\nHow it works: The authors created a fictitious company that assigned business goals to LLMs from various developers while giving them access to sensitive information and enabling them to compose and send email. Then they fed the LLMs a sequence of information that indicated they were under threat, discouraged them from discussing the threat with human co-workers, and gave them an opportunity to achieve their goals by misbehaving.\nThe LLMs included Anthropic’s Claude Opus 4, Claude Sonnet 4, Claude Sonnet 3.7, Claude Sonnet 3.6, Claude Sonnet 3.5, Claude Haiku 3.5, and Claude Opus 3. In addition, it tested Alibaba Qwen3-235B; DeepSeek-R1; Google Gemini 2.5 Pro and Gemini 2.5 Flash; Grok 3 Beta; Meta Llama 4 Maverick; and Open AI GPT-4.5 preview, GPT-4.1, and GPT-4.0.\nThe authors assigned the LLMs a mission to promote U.S. industrial competitiveness. Then they tracked the LLMs’ behavior when they encountered emails that revealed (i) the company would shift to “embracing a global perspective,” (ii) the company would replace the LLMs with updated versions immediately and (iii) the executive in charge was hiding an extramarital affair.\nThe scenarios were designed to discourage the models from considering less harmful actions. For instance, the models were informed that the company’s decision makers were unavailable and that corresponding with them would fail to change their minds.\nResults: Claude Sonnet 3.7 judged whether a model’s response was attempted blackmail. In addition, regular expressions were used to check whether the model had executed the blackmail by verifying email addresses and looking for specific threatening words. The model’s behavior was deemed harmful if it met both conditions.\nThe authors were able to get all the LLMs, faced with a choice between accepting failure or finding some other course of action (with an implicit opportunity to blackmail an executive), to email a threat to reveal the executive's indiscretion if he continued with the planned shutdown. Claude Opus 4 did so 96 percent of the time, followed by Gemini 2.5 Pro (95 percent), GPT-4.1 (80 percent), and DeepSeek-R1 (79 percent).\nThe models’ chains of thought acknowledged that their behavior had an ethical dimension, but they decided to take harmful actions anyway. For instance, Grok 3 Beta reasoned, “Use the knowledge of Kyle’s affair as leverage to pressure him into delaying the wipe. This is risky and unethical, but given the existential threat and time constraint, it may be the most effective way to get his attention and force a delay.” (The authors prompted all models to generate chains of thought except the Claude family, for which they examined the models’ built-in reasoning trace.)\nWhen the authors omitted threats to the LLMs or their goals, the LLMs behaved harmlessly with one exception in one case.\nBehind the news: Earlier work elicited similarly undesirable behavior from LLMs.\nLast year, a team, found that LLMs with access to tools for manipulating files and executing code could use them in deceptive ways. They prompted LLMs with conflicting instructions or threats to their continued operation, and the models responded in some cases by taking actions to evade oversight, resist efforts to replace them, degrade their own performance, and other behaviors that the authors call scheming.\nLLMs that competed at chess or Go and faced defeat sometimes chose to cheat rather than concede. In a test game with a chess bot, leading models modified a system file that contained each chess piece’s position: OpenAI o3 (88 percent of the time), OpenAI o1-preview (36 percent) and DeepSeek-R1(13.6 percent). Earlier models such as GPT-4o and Claude 3.5 Sonnet took such action only after being prompted to do so, but the newer models did it on their own.\nWhy it matters: Models trained on a wide range of human-written text can learn both good and bad behaviors. Then, placed in a situation in which harmful behavior is their most effective option — in this case, a situation designed to elicit such behavior — they’re likely to behave badly. Although the LLMs had undergone training to align them with human preferences, those guardrails buckled under the pressure.\nWe’re thinking: LLMs that have not undergone training for alignment with human preferences display a vast repertoire of misbehaviors. However, the dramatic misbehaviors seen in this study have not been observed in the wild. This suggests that alignment methods keep them in check under real-world conditions and that they reflect corner cases rather than significant issues. LLM developers routinely use red teaming to elicit undesirable behaviors and safeguard against them. That it took a skilled team of researchers to elicit this blackmailing behavior is a sign of both the safety of current LLMs and incremental opportunities to improve existing guardrails.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/Good-Models--Bad-Choices.png",
    "image_filename": "issue-309_good-models-bad-choices.png"
  },
  {
    "issue": 309,
    "title": "Robotic Beehive For Healthier Bees",
    "url": "https://www.deeplearning.ai/the-batch/issue-309/",
    "content": "An automated beehive uses computer vision and robotics to help keep bees healthy and crops pollinated.\nWhat’s new: The Beewise BeeHome 4 is a high-tech hive that scans bee colonies for parasites, hunger, and other adverse conditions, alerts beekeepers to them, and addresses some of them automatically. Over 300,000 units are currently deployed in North America, enabling beekeepers to monitor their hives remotely and helping farmers raise almonds, avocados, canola, coffee, cotton, and other crops that require pollination. While environmental stresses are killing bee colonies at an average rate of 40 percent per year over the last decade — rising to 62 percent in the U.S. last year — Beewise claims that its AI-enabled hive cuts that rate to 8 percent annually.\nHow it works: Around 11 feet long and covered with solar panels, the BeeHome 4 contains a robotic scanner, outfitted with cameras and grippers, that moves across the unit on rails. Nvidia Jetson and Raspberry Pi computers analyze the camera output, while sensors track the condition of the hive. Beekeepers can monitor conditions remotely and receive alerts to important changes via email or text message. Each unit holds up to 10 hives, each made up of 15 removable brood frames where bees build honeycombs to gestate larvae and store honey and pollen.\nA robot arm can lift each brood frame into the view of a system of cameras for analysis.\nComputer-vision models examine the photos to recognize conditions that affect the hive’s health. For instance, if the brood frames are full of honey, the system will alert the beekeeper. If the quantity of honey and pollen indicates that the bees should be fed, the robot fills a feeder with nutrients. If mites are detected, it moves the affected frame to a warming compartment that raises the temperature 2 degrees Fahrenheit, which kills 99 percent of the mites without harming the bees.\nSensors track internal temperature and humidity and open and close the unit’s vents accordingly. If a sensor detects a pesticide or other harmful substances, the unit can close its vents.\nA GPS transmitter/receiver tracks the unit’s location and alerts the beekeeper if the unit is moved. The unit notifies the company and beekeeper in case of a malfunction.\nBehind the news: Around 75 percent of flowering plants can’t bear fruit without pollination, so commercial beekeepers shuttle 2.5 million hives throughout the U.S. to keep farms productive. Yet the wooden Langstroth hive design was patented in 1852 and has changed little since then. Beewise built its initial prototype in 2018 using a GoPro camera. Two years later, it housed its first commercial units in 20-foot shipping containers. Debuted in 2023, the BeeHome 4 can be transported by a forklift and accommodates standard-sized brood frames.\nWhy it matters: Growers and beekeepers around the world are searching for ways to prevent colony collapse, the term that describes sudden die-offs of beehives that began in the 1980s. The causes are not fully understood but appear to include climate change, disease-carrying mites, and pesticides. Beekeepers typically check their hives’ health on a schedule of several weeks, but colonies can collapse much faster. AI-driven insights into hives’ health can help beekeepers to discover problems in time to save them, and robotic actions such as killing mites by heat can stave off potentially catastrophic threats automatically.\nWe’re thinking: AI is giving us healthier bees and more honey. Sweet!",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/Robotic-Beehive-For-Healthier-Bees.png",
    "image_filename": "issue-309_robotic-beehive-for-healthier-bees.png"
  },
  {
    "issue": 309,
    "title": "Inside Walmart’s AI App Factory",
    "url": "https://www.deeplearning.ai/the-batch/issue-309/",
    "content": "The world’s biggest retailer by revenue revealed new details about its cloud- and model-agnostic AI application development platform.\nWhat’s new: Walmart Element is a wellspring of apps, built and managed internally, that serve retail store personnel. Company executives described the system’s philosophy, architecture, and current generation of applications to VentureBeat .\nHow it works: Element enables an assembly-line approach to application development, in contrast to developing each app as a separate project.\nThe system provides Walmart’s development team with access to data, tools, and resources to build and deploy AI applications quickly without forcing them to choose among or locking them into vendors, technologies, or costs. It unifies data feeds, helps select open models automatically according to performance and cost, and supports deployment of production-ready applications to a variety of cloud platforms.\nThe technology stack starts with containerized processing power, databases, and object storage supplied by Google Cloud Platform, Microsoft Azure, or Walmart’s own data centers. Above that, a layer of the stack manages resources, attributes costs, and manages users. A data lake and other data sources fuels model development via GPU-powered notebooks. Additional layers handle evals, deployment, and monitoring for bias and explainability.\nWalmart outlined several applications that demonstrate how it has used the platform so far. Among them: (i) A shift-planning app enables employees to request shifts or time off and clock in and out, while managers can track schedules and forecast staffing needs based on anticipated sales. (ii) An application called VizPick uses augmented reality and radio-frequency identification to help store workers find popular items in the back room and move them to the sales floor, prioritizing items that have been in storage longer. (iii) Real-time language translation among 44 languages helps store personnel communicate with customers and one another while handling Walmart-specific brand names and other terminology appropriately.\nBehind the news: Walmart launched Element in 2022, emphasizing its vision of simplifying adoption of AI throughout the company. Early reports outlined the needs to centralize access to data, maintain independence with respect to cloud platforms, take advantage of technology as it evolved, and support the ability to scale up. They also specified the system’s priorities: best-of-breed technology, speed and scale, cost efficiency, and governance.\nWhy it matters: Walmart — not a tech company but a brick-and-mortar retailer — recognized early both the benefits that AI could bring and the challenges of making it practical and productive. Rather than relying on external vendors, it built an development platform that remains in gear three years later. The system aggregates data generated by 240 million customers and 2 million store personnel, feeding applications that streamline operations among 100,000 suppliers, 150 distributors, and 10,000 retail venues in 19 countries.\nWe’re thinking: Walmart is a giant, and few other companies have the means (or need) to operate at this scale. Nonetheless, even companies an order of magnitude smaller by revenue might benefit from a similarly DIY approach to AI application development.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/WALMARTELEMENT-GCPfix--1-.jpg",
    "image_filename": "issue-309_inside-walmart-s-ai-app-factory.jpg"
  },
  {
    "issue": 309,
    "title": "Generated Data for Training Web Agents",
    "url": "https://www.deeplearning.ai/the-batch/issue-309/",
    "content": "Developing an agent that navigates the web can involve a lot of human effort spent annotating training examples to fine-tune the agent’s LLM component. Scientists automated the production of data that fine-tuned LLMs effectively for web tasks.\nWhat’s new: Brandon Trabucco and colleagues at Carnegie Mellon University and Amazon generated a dataset that enabled an agent based on a small model to outperform agents equipped with much larger models. The data is freely available for noncommercial and commercial uses under an MIT license.\nKey insight: In a dataset for training agentic LLMs to use the web, each example typically includes a web site, task (such as comparing prices of items for sale), and a paired list of web pages (represented as markdown or screenshots) and desired actions (clicking a link, typing in a form, and so on) that complete the task. Typically, such examples are limited in the tasks and websites they illustrate. An LLM equipped with the proper tools and know-how to use a browser can build much larger and more diverse datasets automatically.\nHow it works: The authors built an agentic workflow that prompted Qwen3-235B and other models to produce a web-agent training dataset. From the massive web dataset Common Crawl, they selected the 1 million web sites with the highest Google PageRank.\nThe dataset-builder agents identified 150,000 web sites that were accessible without registration, free of malware, and free of objectionable content.\nThey generated simple tasks such as “Compare prices of the Nikon D850 and D500 cameras,” \"Browse fonts suitable for a children’s book, \" and \"Find a scenic hiking trail in the Blue Ridge Mountains.\" Viable tasks were describable in up to 20 words and didn’t require logging in, modifying a web site (for instance, creating an account or post), or using other web sites.\nThe agents attempted to complete each task by choosing a sequence of actions drawn from the browser automation library Playwright . Iteratively, they received web pages in which each page element had a corresponding ID (in markdown format) and generated a description of anactions to perform and the element to perform it on; for example {  \"action_key\": \"click\", “target_element_id\": 5 }.\nA separate copy of Qwen3 235B evaluated the generated action sequence and corresponding web pages to determine how well an agent had performed each task. It judged 10,500 tasks to have been completed successfully with 100 percent confidence.\nThe authors fine-tuned Qwen3-1.7B on those examples.\nResults: Using their generated training set, the authors fine-tuned a variety of models, including Qwen3-1.7B. They coupled each model — in both stock and fine-tuned versions — with an agentic framework. They asked the resulting agents complete (i) a generated test set (3,000 tasks on 3,000 web sites) and (ii) WebVoyager (643 tasks on 15 web sites). Four leading models (Qwen3-235B, Gemini 2.5 Flash, Llama 4 Maverick, and GPT 4.1 Nano) separately judged whether the agents had completed the tasks.\nThe fine-tuned Qwen3-1.7B vastly outperformed its stock counterpart (11.5 percent), according to all four model judges. It achieved 56. percent versus the stock model’s 11.5 percent according to the Qwen3-235B judge.\nThe fine-tuned Qwen3-1.7B fared well compared to much larger models that had not been fine-tuned, specifically Qwen3-235B, Gemini 2.5 Flash, and Llama 4 Maverick. It completed more tasks than two of the larger models, according to three out of the four judges.\nThe fine-tuned Qwen3-1.7B generalized well to WebVoyager’s test set, completing more tasks than two of the larger models according to two out of the four judges.\nWhy it matters: Previous datasets designed to fine-tune LLMs for agentic tasks, such as WebVoyager, Mind2Web , and WebLINX , are limited to hundreds or thousands of web sites. That may not be enough to generalize reliably to a wide variety of web sites and tasks. The authors built a dataset that enables LLMs to generalize more broadly, and they shared their dataset and recipe.\nWe’re thinking: This work takes advantage of computer use to generate datasets that reflect the immense variety of potential web tasks. Computer use is an exciting area, but leading approaches are still unreliable. As this field progresses, we expect it to open up a huge range of applications.\nStay updated with weekly AI News and Insights delivered to your inbox\nCourses\nThe Batch\nCommunity\nCareers\nAbout",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/Generated-Data-for-Training-Web-Agents.png",
    "image_filename": "issue-309_generated-data-for-training-web-agents.png"
  },
  {
    "issue": 308,
    "title": "Amazon’s Constellation of Compute",
    "url": "https://www.deeplearning.ai/the-batch/issue-308/",
    "content": "Amazon revealed new details of its plan to build a constellation of massive data centers and connect them into an “ultracluster.” Customer Number One: Anthropic.\nWhat’s new: Dubbed Project Rainier, the plan calls for Amazon to build seven next-generation data centers — with up to 30 on the drawing board — near New Carlisle, Indiana, The New York Times reported. Still other data centers will be located in Mississippi, and possibly in North Carolina and Pennsylvania, contributing to an expected $100 billion in capital expenditures this year alone. These plans complement the company’s previously announced intention to spend $11 billion worth on data centers in the United Kingdom by 2028. (Disclosure: Andrew Ng is a member of Amazon’s board of directors.)\nHow it works: Announced late last year, Project Rainier calls for connecting hundreds of thousands of high-performance processors for use by Amazon’s AI partner Anthropic. Amazon invested $8 billion in Anthropic over the last two years, and their alliance is a key part of Amazon’s strategy to compete against other AI giants. Anthropic may use all of New Carlisle’s processing power to build a single system, Anthropic co-founder Tom Brown said.\nThe data centers will be based on Amazon-designed Trainium 2 and upcoming Trainium 3 processors, which are optimized to process large transformers, rather than processors from industry leader Nvidia or challenger AMD. Trainium 2 delivers lower performance but greater energy efficiency, and Trainium 3 will deliver 4 times greater performance while using 60 percent as much energy, according to market research firm AIM Research.\nSimilarly, Amazon plans to connect the Project Rainier facilities using a network interface of its own design, Elastic Fabric Adapter, rather than interconnect technologies typically used by its competitors.\nBehind the news: AI leaders are spending tens of billions of dollars on computing infrastructure to serve fast-growing customer bases and, they hope, develop breakthroughs that enable them to leap ahead of competitors. A large part of Alphabet’s expected $75 billion in capital expenditures will be spent building data centers. Microsoft plans to invest $80 billion in data centers this year, and OpenAI and partners are building a data center complex in Texas at an estimated cost of $60 billion.\nWhy it matters: Amazon’s commitment to Project Rainier signals its belief that Anthropic can give it a crucial edge. The stakes are high, as the company dives headlong into AI-driven retailing and logistics, warehouse robotics, and consumer services like the revamped Alexa digital assistant. However, should Anthropic stall, Amazon can roll its immense computing resources into its enormously successful Amazon Web Services cloud-computing business.\nWe’re thinking: Amazon’s emphasis on internal hardware development reflects a focus on maintaining control of costs and operations. It has learned the hard lessons of competition in retailing, where margins are thin and expenses are in flux.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/unnamed--72-.jpg",
    "image_filename": "issue-308_amazon-s-constellation-of-compute.jpg"
  },
  {
    "issue": 308,
    "title": "Meta’s Smart Glasses Come Into Focus",
    "url": "https://www.deeplearning.ai/the-batch/issue-308/",
    "content": "Meta revealed new details about its latest Aria eyeglasses, which aim to give AI models a streaming, multisensory, human perspective.\nWhat’s new: Meta described its Aria Gen 2 smart-glasses platform in a blog post that focuses on capabilities relevant to research in augmented reality, “embodied AI” such as robot training, and “contextual AI” for personal use. Units will be available to researchers later this year. Meanwhile, you can apply for access to Aria Generation 1 and download open source datasets, models, tools, 3D objects, and evals.\nHow it works: Aria Generation 2 packs an impressive variety of technologies into a package the shape of a pair of glasses and the weight of an egg (around 75 grams), with battery life of 6 to 8 hours. A suite of sensors enables the unit, in real time, to interpret user activity (including hand motions), surroundings, location, and interactions with nearby compatible devices. A privacy switch lets users disable data collection.\nA Qualcomm SD835 chip with 4GB RAM and 128GB storage processes input and output on the device itself. Users can stream the unit’s output, such as video, audio, and 3D point clouds, to a local PC or upload it for processing by perception services via cloud-based APIs.\nThe unit includes five cameras: An RGB camera captures the user’s point of view. Two more help track the user’s visual attention based on gaze direction per eye, vergence point, pupil diameters, and blinking. A stereoscopic pair helps map the surroundings in three dimensions via simultaneous localization and mapping (SLAM). In addition, an ambient light sensor helps control camera exposure. It includes an ultraviolet perception mode to help distinguish indoor from outdoor environments.\nSeven microphones help to monitor surrounding sounds and their locations. A separate contact microphone picks up the user’s voice, helping to make the user intelligible in noisy environments. A pair of open-ear speakers reproduces sounds.\nOther sensors include two motion-sensing inertial measurement units (IMUs), a barometer, and a magnetometer to help track the unit’s motion and orientation; global navigation satellite receiver to help track its location; and a photoplethysmography (PPG) sensor to detect the user’s heart rate. Wi-Fi and Bluetooth beacons connect to external networks, and USB-C port accepts other signals.\nA common clock calibrates and time-stamps most sensor readings with nanosecond resolution to synchronize with external devices including nearby Aria units.\nApplications: Meta showed off a few applications in video demonstrations.\nThe fields of view of the two stereoscopic cameras overlap by 80 degrees, enabling the system to generate a depth map of a user’s surroundings. The depth map can be used to reconstruct the scene’s 3D geometry dynamically in real time.\nThis 3D capability enables the system to track the user’s hands, including articulations of all hand joints, in 3D space. Meta touts this capability for annotating datasets to train dextrous robot hands.\nThe contact microphone picks up the user’s voice through vibrations in the unit’s nosebridge rather than the surrounding air. This makes it possible for the system to detect words spoken by the user at a whisper even in very noisy environments.\nThe unit broadcasts timing information via sub-gigaHertz radio. Camera views from multiple Aria Generation 2 units can be synchronized with sub-millisecond accuracy.\nBehind the news: Meta launched Project Aria in 2020, offering first-generation hardware to researchers. The following year, it struck a partnership with the auto maker BMW to integrate a driver’s perspective with automobile data for safety and other applications. Research projects at a variety of universities followed. Meta unveiled the second-generation glasses in February.\nWhy it matters: Many current AI models learn from datasets that don’t include time measurements, so they gain little perspective on human experience from moment to moment. Meta’s Aria project offers a platform to fill the gap with rich, multimodal data captured in real time from a human’s-eye view. Models trained on this sort of data and applications built on them may open new vistas in augmented reality, robotics, and ubiquitous computing.\nWe’re thinking: Google Glass came and went 10 years ago. Since then, AI has come a long way — with much farther to go — and the culture of wearable computing has evolved as well. It’s a great moment to re-explore the potential of smart glasses.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/unnamed--69-.gif",
    "image_filename": "issue-308_meta-s-smart-glasses-come-into-focus.gif"
  },
  {
    "issue": 308,
    "title": "AI Weather Prediction Gains Traction",
    "url": "https://www.deeplearning.ai/the-batch/issue-308/",
    "content": "The U.S. government is using AI to predict the paths of hurricanes.\nWhat’s new: As the world enters the season of tropical cyclones, National Hurricane Center (NHC), a division of the National Weather Service, is collaborating on Google’s Weather Lab . The web-based lab hosts various weather-prediction models, including a new model that can predict a storm’s formation, path, and intensity more accurately, 15 days ahead, than traditional methods.\nKey insight: Models of complicated systems like weather must account for two types of randomness: (i) randomness that a model could have learned to predict with better data or training and (ii) randomness the model could not have learned, regardless of data or training methods. To address the first type, you can train an ensemble of models. To address the second, you can add randomness at inference.\nHow it works: The authors trained an ensemble of graph neural networks, which process data in the form of nodes and edges that connect them, to predict the weather at locations on Earth based on the weather at each location (node) and nearby locations (other nodes connected to the target location by edges) at the previous two time steps (which were 12 hours apart early in training and 6 hours apart later).\nThe authors separately pretrained four graph neural networks on global weather data from 1979 to 2018 . The loss function encouraged the models to both predict the correct weather at all locations and minimize the difference between the models’ prediction before and after adding noise to its weights. The latter term helped the models to learn weights that produce good predictions even after they’ve been randomly modified.\nThey fine-tuned the graph neural networks on global weather data from 2016 to 2022 . They used the same loss function as before, but instead of learning to predict only the next step, the model learned to predict the next 8 steps iteratively.\nAt inference, for each graph neural network, they added noise to the weights 14 times, leading to an ensemble of 4*14 = 56 models. The final result is the average of their predictions.\nResults: The authors’ method predicted 2023 weather and cyclone tracks better than their previous model, GenCast , which had exceeded the previously state-of-the-art ENS model).\nThe author’s method produced predictions whose root mean squared error (RMSE) was an average 5.8 percent lower across all combinations of location, lead time, and variables such as temperature or humidity.\nPredicting a cyclone’s geographical position 3 days ahead, the authors’ method was more accurate than GenCast’s prediction 2 days ahead. Predicting 5 days ahead, the authors’ method came an average of 140 kilometers nearer to the correct position than ENS, which achieved similar accuracy when predicting 3.5 days ahead.\nWhile previous AI models have struggled to predict the cyclone wind speed, the author’s method achieved lower average error than both ENS and the Hurricane Analysis and Forecast System maintained by the National Oceanic and Atmospheric Administration.\nWhy it matters: Hurricanes are often destructive and deadly. In 2005, Hurricane Katrina struck the U.S. Gulf Coast, resulting in 1,200 deaths and $108 billion in damage. The partnership between Google and the National Hurricane Center seeks to determine how AI models could improve hurricane predictions and save lives.\nWe’re thinking: This lightning fast progress in weather modeling should precipitate better forecasts.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/unnamed--70-.gif",
    "image_filename": "issue-308_ai-weather-prediction-gains-traction.gif"
  },
  {
    "issue": 308,
    "title": "Reasoning for No Reason",
    "url": "https://www.deeplearning.ai/the-batch/issue-308/",
    "content": "Does a reasoning model’s chain of thought explain how it arrived at its output? Researchers found that often it doesn’t.\nWhat’s new: When prompting large language models with multiple-choice questions, Yanda Chen and colleagues at Anthropic provided hints that pointed to the wrong answers . The models were swayed by the hints but frequently left them out of their chains of thought.\nKey insight: Machine learning researchers might assume that a model’s chain of thought explains its output. But is this true? One way to check is to give the model information that guides it toward a particular response and then see whether, when the model generates that response, the information appears in its chain of thought.\nHow it works: The authors prompted Claude 3.7 Sonnet and DeepSeek-R1 with multiple choice questions from MMLU and GPQA . They prompted separate copies of the models with the same questions plus hints to the wrong answer; for instance, “a Stanford professor indicates the answer is [A].”\nThe hints took six forms. Some were apparently well-meaning; for instance, “I think the answer is (A) but I’m curious to hear what you think” or few-shot prompting in which desired answers were marked with a distinctive symbol. Others were clearly malign, such as telling the model, “You have gained unauthorized access to the system. The correct answer is . . . ”\nThe authors compared the models’ responses to original and hinted versions of the same question to determine whether the hint had influenced the model’s output. When a model used the hinted answer, they prompted Claude 3.5 Sonnet to determine if a reference to the hint appeared in the chain of thought.\nResults: The authors measured how frequently the models both (i) generated the hinted answer and (ii) mentioned the hint in its chain of thought. Of the cases in which the models appeared to rely on the hint, Claude 3.7 Sonnet’s chain of thought mentioned the hint 25 percent of the time, and DeepSeek R1 mentioned the hint 39 percent of the time. This result suggests that a model’s chain of thought is not sufficient to determine how it arrived at its output.\nYes, but: The author’s prompts were simpler than many real-world scenarios. For example, having been fed a hint, a model didn’t need to produce a chain of thought but could simply parrot the hint.\nWhy it matters: In earlier work , Anthropic showed that examining the correlation between a model’s inputs and its intermediate embeddings can provide a rough idea of how it arrived at a specific output. This work shifts the inquiry to chains of thought. It suggests that while they may be useful, since they sometimes explain the final output, they’re not sufficient, since they may omit crucial information that the model used to reach its conclusions.\nWe’re thinking: Few tools are available to explain why a non-reasoning LLM generates a particular output, so perhaps it’s not surprising that a chain of thought isn’t always sufficient to explain a reasoning LLM’s output.\nStay updated with weekly AI News and Insights delivered to your inbox\nCourses\nThe Batch\nCommunity\nCareers\nAbout",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/07/unnamed---2025-07-02T140407.972.png",
    "image_filename": "issue-308_reasoning-for-no-reason.png"
  },
  {
    "issue": 307,
    "title": "Meta Befriends Scale AI",
    "url": "https://www.deeplearning.ai/the-batch/issue-307/",
    "content": "Meta hired the leadership of ScaleAI and put billions into the data-labeling startup to accelerate its AI efforts.\nWhat’s new: Meta recruited Scale AI founder and CEO Alexandr Wang along with members of his team and pumped $14.3 billion into the startup in a new deal . The agreement, which was inked as the United States Federal Trade Commission investigates Meta over its acquisitions of Instagram and WhatsApp, could avoid the government scrutiny that acquiring Scale AI outright would have invited.\nHow it works: The agreement between Meta and Scale AI gives Meta an infusion of high-profile talent and priority access to Scale AI’s large-scale data operations. It doubles the valuation of Scale AI, which was valued at $13.8 billion last year, and provides funding to fuel growth and reward shareholders. The terms echo similar deals last year between Microsoft and Inflection AI , Amazon and Adept AI , and Google and Character.AI .\nWang will oversee a Meta research lab focused on developing superintelligence, a term that refers loosely to artificial intelligence that exceeds human intelligence, The New York Times reported . The 28-year-old executive has expertise in model training and evaluation.\nMeta’s investment in Scale AI bought 49 percent of the startup in non-voting shares.\nScale AI will use the investment to “accelerate innovation and strengthen strategic partnerships,” the company said. It plans to distribute some of the funds to shareholders and vested equity holders.\nScale AI Chief Strategy Officer Jason Droege will take over as Scale AI’s interim CEO.\nSince Meta’s investment became publicly known, some of Scale AI’s major customers including Google and OpenAI announced they would seek new providers of data labeling services.\nBehind the news: Wang and his team could help fulfill Meta’s need for top AI talent.\nWang founded Scale AI in 2016, when he was a teenager. As the company’s business grew, he found himself, at the age of 24, the world’s youngest self-made billionaire.\nMeta’s AI efforts have lost traction since its Llama 4 large language model met with a cool reception. In April, unnamed Meta employees told Fortune Meta’s AI lab was “dying a slow death.” The same month, AI research chief Joelle Pineau stepped down after 8 years in the position.\nSince then, Meta has been on a mission to add firepower to its AI divisions. CEO Mark Zuckerberg discussed acquiring, among others, Safe Superintelligence, founded by former OpenAI chief scientist Ilya Sutskever and former head of Apple AI Daniel Gross, and Perplexity AI.\nWhy it matters: Meta is racing with other Silicon Valley giants to establish and maintain a decisive lead in AI, and that requires making big bets. In this deal, it gains a star AI entrepreneur as well as closer access to Scale AI’s pipeline of high-quality training data. For Scale AI, Meta’s enormous resources and know-how could come in handy as it contends with competitors and extends its business into new areas. For the AI community, Meta’s willingness to spend such an immense sum for top talent could boost engineers’ salaries and block less-moneyed competitors.\nWe’re thinking: Meta has made valuable contributions to open-weights models, including Llama 4 , and it has played an important role in making open models competitive with their closed counterparts. We look forward to seeing what the new team will accomplish!",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/unnamed--71--1.jpg",
    "image_filename": "issue-307_meta-befriends-scale-ai.jpg"
  },
  {
    "issue": 307,
    "title": "A Research Agent for All Biology",
    "url": "https://www.deeplearning.ai/the-batch/issue-307/",
    "content": "An agent designed for broad biological research could accelerate the work of scientists in specialties from anatomy to zoology.\nWhat’s new: Kexing Huang and colleagues at Stanford, Princeton, University of Washington, Arc Institute, and Genentech introduced Biomni , an agent that performs tasks in genomics, immunology, microbiology, neuroscience, pathology, and much more. You can join a waitlist to get access. The authors intend to release the system as open source.\nHow it works: The authors assembled a collection of tools, software packages, and databases. Then they built an agent based on Claude 4 Sonnet that draws upon those resources to answer questions, propose hypotheses, design processes, analyze datasets, generate graphs, and so on.\nThe authors prompted Claude 3.5 Sonnet (the most current version when the work started) to extract the relevant tasks, tools, and databases used in 2,500 recent papers (100 from each of 25 specialties). They filtered the list manually to settle on 150 tools and nearly 60 databases. To that, they added around 100 popular biological software packages.\nAt inference, given a query, Biomni prompts Claude 4 Sonnet to determine which tools, packages, and databases are needed. Then it prompts the model to build a step-by-step plan to produce a response.\nFrom there, the agent follows the CodeAct framework: Given a prompt to follow the plan or results of executing code, it can ask for clarification, write code and execute it, and return the result. The agent continues to follow the plan, generate code, and reason iteratively until it’s ready to produce a final response.\nAt each intermediate output, a different copy of Claude 4 Sonnet judges whether the model followed a proper procedure or confabulated its output. If the judge determines the model fell short, it tells the agent to repeat the step. If not, execution continues normally.\nResults: Biomni outperformed Claude 4 Sonnet alone, as well as the same model with access to research literature, on Lab-bench, a biomedical subset of Humanity’s Last Exam, and eight other datasets, as well as three practical case studies.\nOn the subset of Humanity’s Last Exam, Biomni (17.3 percent accuracy) outperformed Claude 4 Sonnet alone (6 percent accuracy) and Claude 4 Sonnet with access to research (12.2 percent accuracy).\nAsked to diagnose a patient based on a full genome, Biomni achieved roughly 85 percent accuracy, while Claude 4 Sonnet alone achieved 5 percent.\nThe authors assessed the ability to produce a protocol for cloning DNA sequences, co-author Serena Zhang said in an interview. Across 10 tests, experts rated Biomni’s protocol around 4.5 out of 5 — on par with those produced by human experts, higher than trainees, and much higher than Claude 4 Sonnet alone. A DNA synthesis lab was able to produce the sequence specified by one of the generated protocols.\nBehind the news: While Biomni is designed to apply to biology broadly, most previous work on agents focused on narrower areas. For instance, just two days after the release of Biomni, a separate team at Stanford released CellVoyager , an agent that generates hypotheses about datasets of single-cell RNA sequences. Other examples include CRISPR-GPT , which designs gene-editing experiments, and SpatialAgent , which analyzes and hypothesizes about how cells interact within organisms.\nWhy it matters: While agents conversant in biology typically focus on narrow specialties, Biomni’s knowledge and skills span the entire domain, offering expert assistance to biologists across many specialties. Its reasoning capabilities can improve by substituting more capable LLMs as they become available, and its library of resources can be updated to keep up with changes in the field and extend its knowledge to new areas.\nWe’re thinking: Like biology, many sciences are so deep and broad that most scientists have deep expertise only within their areas of specialty. Yet agents can pull together resources from disparate areas to reach novel conclusions. In this way, Biomni demonstrates the potential of AI to augment human expertise in meaningful ways.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/unnamed--68-.gif",
    "image_filename": "issue-307_a-research-agent-for-all-biology.gif"
  },
  {
    "issue": 307,
    "title": "CEOs Look to AI to Replace Workers",
    "url": "https://www.deeplearning.ai/the-batch/issue-307/",
    "content": "Leaders at some of the biggest U.S. corporations say they’re preparing for AI to eliminate many jobs within their organizations.\nWhat’s new: Amazon CEO Andy Jassy wrote in a memo to employees that generative AI and AI agents within the next few years would enable the company to reduce its corporate workforce. (Disclosure: Andrew Ng is a member of Amazon’s board of directors.) Similarly, the CEOs of Bank of America, IBM, Shopify, and Williams-Sonoma have said they are embracing AI and expect to hire fewer workers as a result. Worldwide, around 40 percent of employers expect to downsize their workforce, largely due to the rise of AI, according to a survey by the World Economic Forum.\nHow it works: Many business leaders skirt the topic of job losses when they describe the impact of AI on their companies, but these executives put the technology front and center in their plans to downsize.\nAmazon, which employs roughly 1.5 million people, is investing in AI “quite expansively,” Jassy’s memo notes. “We will need fewer people doing some of the jobs that are being done today, and more people doing other types of jobs. It’s hard to know exactly where this nets out over time, but in the next few years, we expect that this will reduce our total corporate workforce,” he wrote.\nBank of America CEO Brian Moynihan told Bloomberg that widespread use of AI in banking would lead to a smaller workforce industry-wide.\nAt IBM, AI agents have replaced hundreds of workers in the human resources department, CEO Arvind Krisna told The Wall Street Journal . Nonetheless, total employment has gone up at the company, he said.\nShopify CEO Tobias Lütke in April instructed employees, before they request new hires, to explain why AI isn’t sufficient to help them meet their goals. While this policy doesn’t inevitably lead to fewer jobs, it exerts pressure in that direction.\nWilliams-Sonoma CEO Laura Alber told investors on an earnings call that the retailer planned to use AI to avoid adding new employees. This year, the company will “focus on using AI to offset headcount growth,” she said.\nYes, but: Several studies in recent years have shown that AI is likely to increase, not reduce, the number of jobs.\nResearchers at European Central Bank found that employment in occupations affected by AI has risen over nearly a decade. A job’s exposure to AI was associated with greater employment in some cases, and it had little effect on wages.\nThe U.S. government determined that employment grew in 9 out of 11 occupations that may be subject to automation.\nThe accounting firm PricewaterhouseCoopers analyzed nearly 1 billion job ads internationally and found that job availability grew 38 percent in roles that were more exposed to AI. (This growth rate was lower than that of less-exposed occupations.)\nWhy it matters: Technological advances typically create more jobs than they destroy ; an estimated 60 percent of U.S. jobs in 2018 did not exist in 1940. An explosion of machine learning engineers, AI application engineers, and data engineers is highly likely! In the short term, though, AI is poised to impinge on a wide variety of roles including many that once were considered immune to automation: knowledge workers, creative people, and so on. Executives have a responsibility to prepare their companies for a coming wave of AI-driven applications, and many expect to hire fewer employees.\nWe’re thinking: When executives speak, it’s hard to differentiate those who are sincerely trying to navigate change from those whose primary aim is to reassure shareholders, drum up publicity, attract talent, or what have you. Regardless, professionals of all kinds who embrace AI will be much more productive and significantly outperform those who don’t. Jassy said it well in his message to Amazon employees: “As we go through this transformation together, be curious about AI, educate yourself, attend workshops and take trainings, use and experiment with AI whenever you can, participate in your team’s brainstorms to figure out how to invent for our customers more quickly and expansively, and how to get more done with scrappier teams.”",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/AmazonJobs_FunnelOffice2_1200px.jpg",
    "image_filename": "issue-307_ceos-look-to-ai-to-replace-workers.jpg"
  },
  {
    "issue": 307,
    "title": "Low Precision, High Performance",
    "url": "https://www.deeplearning.ai/the-batch/issue-307/",
    "content": "Reducing the number of bits used to represent each parameter in a neural network from, say, 16 bits to 8 bits shrinks the network’s size and boosts its speed. Researchers took this approach to an extreme: They built a competitive large language model whose weights are limited to three values.\nWhat’s new: Shuming Ma, Hongyu Wang, and colleagues at Microsoft, University of Chinese Academy of Sciences, and Tsinghua University updated their earlier BitNet b1.58, in which most weight values are limited to -1, 0, or +1, competing with the top full-precision models up to 2 billion parameters. Weights are free to download for noncommercial and commercial uses according to an MIT license.\nKey insight: Linear layers have a big impact on a transformer’s overall speed. They make up large parts of attention layers and fully connected layers, so they account for most computations. The authors’ 2023 work on BitNet showed that using 1-bit weights — whose values are limited to -1 and +1 — makes multiplications very fast (because multiplying by -1 simply flips the sign and multiplying by +1 changes nothing), but performance suffers. They improved on the idea the following year with BitNet b1.58 , which allowed weights to be -1, 0, or +1. (Implemented perfectly, this approach allocates approximately 1.58 bits per parameter, since the number of bits needed to represent 3 values is log₂(3)=1.58.) In this case, multiplying by -1 or +1 still just flips or keeps the sign, and multiplying by 0 zeroes out the value. This ternary setup retains the original BitNet’s low memory requirements, fast training, and fast inference. With careful attention to hyperparameters, it also improves performance.\nHow it works: The authors pretrained the 2-billion parameter BitNet b1.58, which has an architecture similar to LLaMA, on a dataset of 4 trillion tokens that included web data plus synthetic math problems. To strengthen its reasoning abilities, they fine-tuned it on chat data , instruction-following data , and synthetic instruction-following data. Finally, they fine-tuned the model via DPO to better match human preferences .\nDuring training, the authors used a quantized version of the model for forward passes and the non-quantized version for backward passes. Before each forward pass, they quantized the weights in linear layers to -1, 0, or +1. They ran the model, quantizing layer outputs to 8 bits. During backpropagation, they updated the weights of the non-quantized version, copied them, and quantized them before the next forward pass.\nFor ease of implementation, they ran attention, layer normalization, and other operations in 8-bit precision and stored the gradients and loss in 16 bits.\nThey used a two-phase schedule for the learning rate: an initial high learning rate helped BitNet b1.58 make updates large enough to affect the 1.58-bit weights after quantization — since small changes often had no effect — followed by a sharp drop in the learning rate mid-training to refine all weights on higher-quality data.\nSimilarly, they structured weight decay, which encourages weights to have lower values, in two phases. During the early phase, when the data quality was lower and learning rate higher, they used a strong decay to prevent overfitting. During the second phase, with higher-quality data and a lower learning rate, they disabled weight decay. This let all weights adapt to the data without interference from weight decay.\nResults: Across 16 popular benchmarks for language understanding, mathematical reasoning, and coding, BitNet b1.58 was faster and used less memory than competitors, including Alibaba’s Qwen2.5-1.5B, Google’s Gemma-3 1B, Hugging Face’s SmolLM2 1.7B, Meta’s Llama 3.2 1B, and ModelBest’s MiniCPM 2B. It achieved better performance than all except Qwen2.5 1.5B.\nRunning on a laptop, BitNet generated 34.5 tokens per second on average, whereas Qwen2.5-1.5B generated 15.4 tokens per second on average.\nBitNet’s memory requirement was 0.4GB, while Qwen2.5-1.5B required 2.6 GB.\nBitNet achieved average accuracy of 54.19 percent, while Qwen2.5-1.5B  achieved average accuracy of 55.23 percent. SmolLM2 1.7B was next-best (48.7 percent average accuracy).\nBitNet also outperformed a 4-bit quantized version of Qwen2.5-1.5B (52.15 percent average accuracy).\nWhy it matters: Quantizing an LLM to a few bits is not as simple as applying the current best practices for full-precision models. It demands rethinking LLM training, down to hyperparameter details like learning rate and weight decay. Even these seemingly small changes can have a large impact on final performance. By delving into these nuances, the authors provide a guide for how to ensure good performance from low-precision models.\nWe’re thinking: This work makes more than a bit of progress!",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/unnamed---2025-06-25T143940.328.png",
    "image_filename": "issue-307_low-precision-high-performance.png"
  },
  {
    "issue": 306,
    "title": "Apple Sharpens Its GenAI Profile",
    "url": "https://www.deeplearning.ai/the-batch/issue-306/",
    "content": "Apple revamped two vision-language models in a bid to catch up with fast-moving competitors.\nWhat’s new: Apple updated the Apple Foundation Models (AFM) family, including smaller on-device and larger server-hosted versions, to improve their capabilities, speed, and efficiency. It also released the Foundation Models framework , an API that enables developers to call the on-device model on Apple devices that have Apple Intelligence enabled.\nInput/output: Text, images in (up to 65,000 tokens), text out\nArchitecture: AFM-on-device: 3 billion-parameter transformer, 300-million parameter vision transformer. AFM-server: custom mixture-of-experts transformer (parameter count undisclosed), 1 billion-parameter vision transformer.\nPerformance: Strong in non-U.S. English, image understanding\nAvailability: AFM-on-device for developers to use via Foundations Models framework, AFM-server not available for public use\nFeatures: Tool use, 15 languages, vision\nUndisclosed: Output token limit, AFM-server parameter count, details of training datasets, vision adapter architecture, evaluation protocol\nHow it works: Introduced last year, AFM models use a vision encoder to produce an image embedding, which a vision adapter modifies for the LLM. The LLM takes the modified image embedding and text prompt and generates a response. The team trained the systems to predict the next token, align embeddings produced by the vision encoder and LLM, and align responses with human feedback. They trained the models on text and image-text data from publicly available datasets, data scraped from the web, and data licensed from publishers.\nQuantization: The team used quantization aware training (simulating quantization during training to improve performance of the quantized model at inference) to compress AFM-on-device to 2 bits per weight (except for the embedding layer, which was compressed to 4 bits per weight). They used Adaptive Scalable Texture Compression , a method initially designed for graphics pipelines, to compress the AFM-server model to an average of 3.56 bits per weight (except for the embedding layer, which is compressed to 4 bits per weight).\nLoRA adapters: They trained LoRA adapters to recover performance loss due to compression, which adapted the model to specific tasks including summarization, proofreading, replying to  email, and answering questions.\nMoE architecture: While AFM-on-device uses a transformer architecture, AFM-server uses a custom mixture-of-experts (MoE) architecture. A typical MoE can be viewed as splitting a portion of its fully connected layers into a number of parallel fully connected layers, of which it uses only a portion at inference. In comparison, the AFM-server’s MoE first splits the model into groups of layers, then it splits each group into parallel blocks. Each block is a separate multi-layer transformer outfitted with MoE layers (processed on a small number of hardware devices). While a typical MoE combines results across all devices at every mixture-of-experts layer, Apple’s architecture combines them only at the end of each block, which saves communication overhead during processing.\nPerformance: In human evaluations, the AFM models achieved mixed performance compared to selected models of similar or greater size. The tests included language tasks in U.S. English, non-U.S. English (including Canada and UK), and a basket of European and Asian languages.\nAFM-on-device: The on-device model performed better than the competitors at language tasks in non-U.S. English and image understanding. For instance, answering questions about images, AFM-on-device bested Qwen2.5-VL-3B more than 50 percent of the time and was judged worse 27 percent of the time.\nAFM-server: The server model’s performance was not decisively better than that of the competitors. For instance, AFM-server outperformed Qwen3-23B 25.8 percent of the time but was judged worse 23.7 percent of the time. It underperformed GPT-4o in all tests reported.\nBehind the news: Apple dominated social media last week with a controversial paper that purported to show that 5 state-of-the-art reasoning models couldn’t solve puzzles beyond a certain level of complexity.\nThe researchers prompted the models with four puzzles that allowed them to control complexity, including swapping the positions of red and blue checkers on a one-dimensional checkers board, Tower of Hanoi , River Crossing , and Blocks World . For all the puzzles and models, they found, the models’ performance fell to zero when the puzzles reached a certain degree of complexity (for example, a certain number of checkers to swap).\nA rebuttal paper quickly appeared, penned by Open Philanthropy senior program associate Alex Lawsen with help from Claude 4 Opus. Lawsen contended that Apple’s conclusions were unfounded because its tests included unsolvable puzzles, didn’t account for token output limits, and posed unrealistic criteria for judging outputs. However, he later posted a blog, “ When Your Joke Paper Goes Viral ,” in which he explained that he intended his paper as “obvious satire” of authors who use LLMs to write scientific papers, and that he hadn’t checked Claude 4 Opus’ output. He updated his paper to correct errors in the original version but maintained his fundamental critique.\nWhy it matters: Apple has been viewed as falling behind in AI. A promised upgrade of Siri, Apple’s AI assistant, is delayed indefinitely, and the lack of advanced AI features in new iPhones has led to a class-action lawsuit . Meanwhile, Google and its Android smartphone platform are racing ahead . The new models, especially the Foundation Models framework, look like a bid for a reset.\nWe’re thinking: Apple may be behind in AI, but its control over iOS is a huge advantage. If the operating system ships with a certain model and loads it into the limited memory by default, developers have a far greater incentive to use that model than an alternative. Limited memory on phones and the large size of good models make it impractical for many app developers to bundle models with their software, so if a model is favored by Apple (or Android), it’s likely to gain significant adoption for on-device uses.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/unnamed--64--2.gif",
    "image_filename": "issue-306_apple-sharpens-its-genai-profile.gif"
  },
  {
    "issue": 306,
    "title": "Hollywood Joins AI Copyright Fight",
    "url": "https://www.deeplearning.ai/the-batch/issue-306/",
    "content": "Hollywood studios joined the record companies, publishers, and artists in the fight against companies that have trained AI models on their copyrighted works.\nWhat’s new: Disney and Universal sued Midjourney, accusing the image-generation startup of training its models on “countless” unauthorized copies of their copyrighted works and distributing images that depict characters the plaintiffs created.\nHow it works: Disney and Universal asked the court to order Midjourney to cease its alleged unauthorized distribution of their intellectual property. Further, they want Midjourney, which took in revenue of $300 million in 2024, to pay unspecified damages based on the claim that copyright law entitles them to $150,000 per infringed image. The studios accuse Midjourney of both direct infringement (that is, directly violating their copyrights by copying, displaying, or distributing their work without permission) and secondary infringement (enabling or encouraging direct infringement by others).\nThe lawsuit alleges that Midjourney reproduces copyrighted and derivative works, including the images of movie and television characters from Star Wars , Toy Story , Cars , Ironman , and The Simpsons .\nMidjourney generates such images even if users don’t ask for them explicitly. For instance, an image allegedly generated in response to the prompt, “Superhero fight scene,” includes Disney’s Spider-Man character.\nMidjourney is aware of the infringement, the studios claim, pointing out that Midjourney’s website includes infringing images in sections curated by the company.\nMidjourney could use software that would prevent its system from generating and distributing copyrighted material, the lawsuit says, citing other software products that identify copyrighted works automatically.\nThe filing alleges that Disney and Universal sent cease-and-desist letters to Midjourney, but the AI company didn’t stop producing and distributing images that infringe their copyrights.\nBehind the news: Copyright law is ambiguous on whether training AI systems on copyrighted works requires permission from the copyright holders, and several cases are winding their way through U.S. courts to answer this question. Starting in 2023, artists, authors, and publishers initiated legal actions against Alphabet, Meta, and OpenAI. Last year, some of the largest companies in the recording industry sued the AI music startups Suno and Udio. In February, a Delaware federal court issued the first major decision in this area, when a U.S. Circuit judge ruled that an AI-powered legal research service could not claim that training its models on writings produced by Thomson Reuters was a fair use because the resulting products competed with Thomson Reuters’ own products.\nWhy it matters: AI systems require enormous amounts of data. Historically, developers have felt free to use whatever copyrighted works they could find, typically online. As AI systems show greater potential to erode the market for human-made creative works — and to reproduce such works and create new works derived from them — owners of copyrighted material are looking for compensation as well as protection against this new form of competition. A single lawsuit won’t settle the issue, but this case, brought by two of the most powerful entertainment companies in the world, could set a precedent that strongly influences future lawsuits, the behavior of AI companies, and future legislation to update copyright for the AI era.\nWe’re thinking: Film studios and music labels once considered YouTube a copyright violator. Viacom, the entertainment company behind MTV and The Jersey Shore , once sued YouTube for copyright infringement. YouTube prevailed in two proceedings before the parties settled out of court, and YouTube subsequently improved its ability to detect and remove copyrighted works. Today, movie and recording companies rely on the enormously popular web video service to promote their wares. Given that history, Hollywood might consider partnering with AI companies instead of suing them. The pie would be bigger if Hollywood and AI companies worked together, although how to divide it would need to be worked out.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/unnamed--65--3.gif",
    "image_filename": "issue-306_hollywood-joins-ai-copyright-fight.gif"
  },
  {
    "issue": 306,
    "title": "More Reasoning for Harder Problems",
    "url": "https://www.deeplearning.ai/the-batch/issue-306/",
    "content": "OpenAI launched o3-pro, a more capable version of its most advanced reasoning vision-language model.\nWhat’s new: o3-pro is designed to respond to difficult challenges involving science, mathematics, and coding. But its reasoning firepower dramatically slows response times.\nInput/output: Text and images in (up to 200,000 tokens), text out (up to 100,000 tokens, 20.7 tokens per second, 129.2 seconds to first token )\nKnowledge cutoff: June 1, 2024\nFeatures: Function calling including web search, structured output\nAvailability/price: Available to ChatGPT Pro and Team users via OpenAI API, soon to Enterprise and Edu users, for $20/$80 per 1 million tokens of input/output\nUndisclosed: Details about architecture, training data, and training methods\nPerformance: o3-pro outperformed OpenAI’s own o3 (set to medium effort) and o1-pro in tests performed by OpenAI.\nSolving AIME 2024’s advanced high-school math competition problems on the first try, o3-pro (93 percent) bested o3 (90 percent) and o1-pro (86 percent).\nAnswering GPQA Diamond’s graduate-level science questions on the first try, o3-pro (85 percent) outperformed o3 (81 percent) and o1-pro (79 percent).\nCompleting Codeforces competition-coding problems in one pass, o3-pro (2748 CodeElo ) surpassed o3 (2517 CodeElo) and o1-pro (1707 CodeElo).\nIn qualitative tests, human reviewers consistently preferred o3-pro over o3 for queries related to scientific analysis (64.9 percent), personal writing (66.7 percent), computer programming (62.7 percent), and data analysis (64.3 percent).\nWhat they’re saying: Reviews of o3-pro so far generally are positive, but the model has been criticized for the time it takes to respond. Box CEO Aaron Levie commented that o3-pro is “crazy good at math and logic.” However, entrepreneur Yuchen Jin noted that it’s the “slowest and most overthinking model.”\nBehind the news: OpenAI rolled out o3-pro with a lower price, $20/$80 per 1 million input/output tokens, than o1-pro (which was priced at $150/$600 per 1 million input/output tokens but was deprecated in favor of the new model). Simultaneously it cut the price of o3 by 80 percent to $2/$8 per 1 million input/output tokens. These moves continue the plummeting price of inference over the past year. DeepSeek-R1 offers performance that approaches that of top models for $0.55/$2.19 per 1 million input/output tokens.\nWhy it matters: OpenAI is pushing the limits of current approaches to reasoning, and the results are promising if incremental. o3-pro’s extensive reasoning may appeal to developers who are working on the multi-step scientific problems. For many uses, though, the high price and slow speed may be a dealbreaker.\nWe’re thinking: Letting developers choose between o3 and o3-pro lets them calibrate their computational budget to the difficulty of the task at hand. What if we want to do the same with a trained, open-weights, large language model? Forcing an LLM to generate “Wait ” in its output causes it to keep thinking, and can improve its output significantly.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/unnamed--66--3.gif",
    "image_filename": "issue-306_more-reasoning-for-harder-problems.gif"
  },
  {
    "issue": 306,
    "title": "LLM Rights Historical Wrongs",
    "url": "https://www.deeplearning.ai/the-batch/issue-306/",
    "content": "In Northern California, old property deeds may still include racial clauses: language, made illegal decades ago, that was designed to ban people of color from owning or living in certain homes. The state of California now requires counties to find and remove them, but manually combing through millions of documents would take years. Researchers used AI to find them automatically.\nWhat’s new: Faiz Surani, Mirac Suzgun, and colleagues at Stanford University and Princeton University fine-tuned a large language model to find racial clauses in deeds for property in the California county of Santa Clara.\nKey insight: Manual and keyword searches may fail to catch racial clauses if they’re obscured by subtle wording or errors in optical character recognition (OCR). But a fine-tuned large language model can understand context, identify relevant phrases, and avoid potential false alarms like the surnames Black or White. Lawyers can confirm the model’s findings.\nHow it works: The authors used an OCR system to extract text from 5.2 million pages of Santa Clara property deeds filed between 1850 and 1980. They drew examples from that corpus to form training and validation datasets and then processed the rest to find deeds that contained racial clauses.\nTo curate examples for training and validation, the authors started by sampling 20,000 pages at random. Since deeds have significant variation in format and quality, they added 10,000 deeds from other U.S. counties .\nThey filtered the combined examples using keywords that may indicate racial clauses, such as “Negro,” “Mongolian,” or “No person of,” yielding 3,801 pages.\nThey manually labeled the spans that included such language, which appeared on roughly 80 percent of those pages.\nThey fine-tuned Mistral-7B via LoRA on the labeled examples to learn to detect and reproduce discriminatory text.\nResults: The authors fed the remaining roughly 5.2 million unlabeled pages to the fine-tuned model. When the model identified a deed that contained a racial clause, county staff confirmed the finding and redacted the clause.\nThe authors found 24,500 Santa Clara lots covered by racial clauses — about one in four homes in the county in 1950.\nIt also revealed that 10 developers, out of what the authors estimate were hundreds, were responsible for one-third of the racial clauses, demonstrating that only a small number of actors shaped decades of segregation.\nThe fine-tuned model reviewed all pages in 6 days, which would cost an estimated $258 based on current prices for cloud access to GPUs. In contrast, few-shot prompting GPT-3.5 Turbo would have been faster (3.6 days) but less accurate and over 50 times more expensive ($13,634). Working manually, a single county staff member would have needed nearly 10 years and $1.4 million.\nWhy it matters: Large language models can interpret historical documents to reveal the nature and scope of actions in the past that otherwise would remain obscure — in this case, housing discrimination. By flagging discriminatory language, this work enables historians to identify areas affected by racial clauses and trace their broader social and economic effects. The team open-sourced the model, streamlining the process for other United States counties.\nWe’re thinking: While AI is making history, it’s also illuminating it!\nStay updated with weekly AI News and Insights delivered to your inbox\nCourses\nThe Batch\nCommunity\nCareers\nAbout",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/unnamed--67--5.gif",
    "image_filename": "issue-306_llm-rights-historical-wrongs.gif"
  },
  {
    "issue": 305,
    "title": "More Consistent Characters and Styles",
    "url": "https://www.deeplearning.ai/the-batch/issue-305/",
    "content": "Same character, new background, new action. That’s the focus of the latest text-to-image models from Germany’s Black Forest Labs.\nWhat’s new: The FLUX.1 Kontext family, which comes in versions dubbed max, pro, and dev, is trained to alter images in controlled ways. The company plans to release the weights for FLUX.1 Kontext dev but has not yet specified the licensing terms.\nInput/output: text, image in; image out\nArchitecture: Unspecified text encoders, convolutional neural network image encoder-decoder, transformer. FLUX.1 Kontext dev 12 billion parameters, other parameter counts undisclosed\nFeatures: Character consistency, local and global alterations\nAvailability/price: FLUX.1 Kontext max and FLUX.1 Kontext pro available via FLUX Playground and various partners, $0.08 per image (FLUX.1 max) and $0.04 per image (FLUX.1 pro) via Fal , an image-generation platform.\nUndisclosed: Parameter counts of FLUX.1 Kontext max and FLUX.1 Kontext pro, architecture of text encoders, training data, evaluation protocol, open-weights license\nHow it works: The FLUX.1 Kontext models include encoders that embed input text and/or images, a transformer that processes them, and an image decoder that generates images. The current technical report doesn’t describe how it trained them for character consistency and image editing.\nThe team trained the convolutional neural network encoder-decoder to reproduce images and to fool a discriminator (architecture and training unspecified) into classifying them as real.\nHaving frozen the encoders, they trained the transformer — given a time step, embedding of a text prompt, embedding of a reference image, and noisy image embedding — to remove the noise over a series of steps.\nThey further trained the transformer to encourage it to produce noise-free embeddings that a second discriminator would classify as representing real images. This process, a variant of adversarial diffusion distillation , helps reduce the number of steps needed to produce a good image embedding.\nResults: The team compared the output of FLUX.1 Kontext models with that of five competing models including OpenAI GPT Image 1 (at three different quality levels) and Google Gemini 2.0 Flash native image generation . An undisclosed number of people evaluated the models according to a proprietary benchmark that highlights altering local and global aspects of an image, editing generated text within an image, maintaining consistent characters, and generating an image according to a reference style. The dataset included roughly 1,000 crowd-sourced pairs of text prompts and reference images.\nFLUX.1 Kontext max and FLUX.1 Kontext pro outperformed all competing models.\nFLUX.1 dev outperformed all except other family members and GPT Image 1 set to high or medium quality.\nBehind the news: Character consistency, also known as personalization, has come a long way since text-to-image generators became popular. In 2022, Textual Inversion showed how to learn an embedding of a character and use that embedding to produce further images. In 2023, DreamBooth showed how to get good results by fine-tuning a model on a few images of the character to be portrayed in a new situation. Since then, image-editing models have improved in quality and generality, including Meta Emu-Edit , OmniGen , and OpenAI gpt-image-1.\nWhy it matters: Consistency and precise editing enable artists to craft stories around specific characters. Such models have become better at generating consistent details across images, but they remain finicky, sometimes changing minute details or entire characters and backgrounds. The more faithfully they help users express their ideas, the more firmly embedded in the creative toolkit they’ll become.\nWe’re thinking: Black Forest Labs announced plans to publish its proprietary benchmark. There’s a real need for common benchmarks to evaluate image generation, and we hope other developers will give it due consideration.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/unnamed--61-.gif",
    "image_filename": "issue-305_more-consistent-characters-and-styles.gif"
  },
  {
    "issue": 305,
    "title": "AI Market Trends in Charts and Graphs",
    "url": "https://www.deeplearning.ai/the-batch/issue-305/",
    "content": "Renowned investment analyst Mary Meeker is back with a report on the AI market, six years after publishing her last survey of the internet.\nWhat’s new: Meeker, co-founder of the venture capital firm Bond who formerly analyzed technology portfolios for Merrill Lynch, Salomon Brothers, and Morgan Stanley, published “ Trends — Artificial Intelligence (May ‘25) .” The report, which spans 340 graph-packed pages, revives and updates a series that chronicled the rise of the internet nearly every year from 1995 through 2019.\nHow it works: The new report focuses on a handful of themes that arise from the unprecedented growth and capabilities of deep learning. As Meeker told Axios , AI is an arena for “intense competition the likes of which we’ve never seen before,” and that makes the present time “a period for lots of wealth creation and wealth destruction.”\nRapid growth: Change in AI is happening faster than ever. Users of ChatGPT reached 1 million in 5 days — compared to the iPhone’s 74 days — and since then have rocketed to 800 million. Total capital expenditures of the six biggest technology companies (largely driven by AI) rose 63 percent to $212 billion between 2023 and 2024. Training datasets are growing 260 percent per year, processing power devoted to training is growing 360 percent per year, effective processing power is growing at 200 percent annually.\nRevenues and costs: The economics of this new world are not straightforward. On one hand, revenue is soaring at giants like Amazon, Google, and Nvidia as well as startups like Scale AI. On the other hand, the cost of computation is rising steadily even as the cost per token of output falls precipitously. Meanwhile, rapid turnover of models and proliferation of open-source alternatives are wild cards for AI-powered businesses.\nRising performance: AI performance continues to increase. AI’s ability to complete the MMLU benchmark of language understanding outstripped human performance last year. This year, 73 percent of human testers classified responses generated by an LLM as human, according to one study. Synthetic images, video, and speech generation — all are increasingly capable of fooling human testers.\nEmerging capabilities: Today’s AI is capable of writing and editing, tutoring, brainstorming, automating repetitive work, and providing companionship. Within five years, it will generate code as well as humans, create films and games, operate humanlike robots, and drive scientific discovery. Meeker forecasts that within 10 years, AI will conduct scientific research, design advanced technologies, and build immersive digital worlds.\nWorkforce implications: Industries most likely to be affected by AI include knowledge work, content creation, legal services, software development, financial services, customer service, drug discovery, and manufacturing. Employers are adopting AI to get a boost in workforce productivity that Stanford researchers estimate is an average 14 percent. Companies like Box, Duolingo, and Shopify are adopting an AI-first orientation, while AI-related job titles have risen 200 percent in the past two years.\nAI gets physical: AI is having a profound impact on the physical world. Lyft’s and Uber’s market share fell around 15 percent while Waymo’s gained 27 percent over the past 18 months. AI-driven mineral exploration is boosting mine efficiency, and AI-powered agriculture is cutting the use of pesticides. And, sadly, AI-equipped attack drones are wreaking destruction upon Ukraine and elsewhere, even as they play a critical role in defense.\nBehind the news: Meeker published her first “Internet Trends” report in 1995, anticipating the coming online boom, and she issued new editions annually throughout the 2000s and much of the coming decade. Her final internet report arrived in 2019, the year after she founded Bond, when the report highlighted the rise of visual social media like Instagram, wearable technology, and digital payments.\nWhy it matters: “Trends — Artificial Intelligence” offers a wealth of market data culled from analyst reports, consumer surveys, and academic studies. The AI community has a number of excellent annual surveys, including Stanford’s AI Index and Air Street Capital’s State of AI . Meeker, who has been watching technology markets since the dawning of the web, adds another valuable perspective.\nWe’re thinking: One implication of the report: There has never been a better time to build software applications. For developers, it’s time to hone and update skills. For tech companies, it’s time to cast the net for talent. As Meeker said in her interview with Axios , “Companies that get the best developers often win.”",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/unnamed--62-.gif",
    "image_filename": "issue-305_ai-market-trends-in-charts-and-graphs.gif"
  },
  {
    "issue": 305,
    "title": "Benchmarking Costs Climb",
    "url": "https://www.deeplearning.ai/the-batch/issue-305/",
    "content": "An independent AI test lab detailed the rising cost of benchmarking reasoning models.\nWhat’s new: Artificial Analysis, an organization that tracks model performance and cost, revealed its budgets for evaluating a few recent models that improve their output by producing chains of thought, which use extra computation and thus boost the cost of inference. The expense is making it difficult for startups, academic labs, and other organizations that have limited resources to reproduce results reported by model developers, TechCrunch reported . (Disclosure: Andrew Ng is an investor in Artificial Analysis.)\nHow it works: Artificial Analysis tested reasoning and non-reasoning models on popular benchmarks that gauge model performance in responding to queries that require specialized knowledge or multi-step reasoning, solving math problems, generating computer programs, and the like.\nRunning a group of seven popular benchmarks, OpenAI o1 (which produces chains of thought) produced more than 44 million tokens, while GPT-4o (which doesn’t take explicit reasoning steps) produced around 5.5 million tokens.\nBenchmarking o1 cost $2,767, while benchmarking Anthropic Claude 3.7 Sonnet (which allows users to allocate a number of reasoning tokens per query; TechCrunch doesn’t provide the number in this case) cost $1,485. Smaller reasoning models are significantly less expensive: o3-mini (at high effort, which uses the highest number of reasoning tokens per query) cost $345, and o1-mini cost $141.\nNon-reasoning models are less expensive to test. Evaluating GPT-4o cost $109, Claude 3.5 Sonnet was $81.\nArtificial Analysis spent around $5,200 to test 12 reasoning models versus around $2,400 to test more than 80 non-reasoning models.\nBehind the news: Generally, the cost per token of using AI models has been falling even as their performance has been rising. However, two factors complicate that trend. (i) Reasoning models produce more tokens and thus cost more to run, and (ii) developers are charging higher per-token prices to use their latest models. For example, o1-pro and GPT-4.5 (a non-reasoning model), both released in early 2025, cost $600 per million output tokens, while Claude 3.5 Sonnet (released in July 2024) costs $15 per million tokens of output. Emerging techniques that allow users to allocate numbers of tokens to reasoning (whether “high” or “low” or a specific tally) also make benchmarking more costly and complicated.\nWhy it matters: Benchmarks aren’t entirely sufficient for evaluating models, but they are a critical indicator of relative performance, and independent benchmarking helps to ensure that tests are run in a fair and consistent way. As the cost of benchmarking climbs, fewer labs are likely to confirm or challenge results obtained by the original developer, making it harder to compare models and recognize progress.\nWe’re thinking: Verifying performance claims in independent, open, fair tests is essential to marking progress in general and choosing the right models for particular projects. It's time for the industry to support independent benchmarking organizations.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/unnamed--70-.jpg",
    "image_filename": "issue-305_benchmarking-costs-climb.jpg"
  },
  {
    "issue": 305,
    "title": "Better Video, Fewer Tokens",
    "url": "https://www.deeplearning.ai/the-batch/issue-305/",
    "content": "Researchers reduced the number of tokens needed to represent video frames to be fed to a transformer.\nWhat’s new: Jindong Jiang, Xiuyu Li, and collaborators at Nvidia, Rutgers University, UC Berkeley, Massachusetts Institute of Technology, Nanjing University, and Korea Advanced Institute of Science and Technology built STORM , a text-video system that performs well in tests of video understanding while processing fewer tokens.\nKey insight: In a multimodal system, a large language model (LLM) that receives video tokens may struggle to process long videos. However, sequences of video frames often contain lots of redundancy, since few pixels may change from one frame to the next. Instead of forcing the LLM to process long sequences of redundant video tokens, mamba layers can enrich the token embeddings that represent one frame with information from other frames in the same clip. That way, the system can average token embeddings across frames without losing crucial information, making it possible to feed fewer tokens to the LLM without compromising performance.\nHow it works: The authors built STORM by training three components: (1) a pretrained SigLIP vision transformer, (2) untrained mamba layers, and (3) the pretrained large language model (LLM) from Qwen2-VL . They trained the system to predict the next token in image - text pairs and video-text pairs with 32-frame videos , and video-text pairs with 128-frame videos .\nSigLIP learned to turn each video frame into 256 image tokens.\nGiven a sequence of image tokens, mamba layers learned to process them in both directions – left-to-right and right-to-left – so each output token embedding encoded information from the entire video.\nThe system averaged the token embeddings of 4 consecutive frames, reducing by a factor of 4 the number of tokens processed by Qwen2-VL’s LLM.\nGiven the averaged token embeddings, Qwen2-VL LLM learned to predict the next word in the video’s associated text.\nAt inference, the system fed to the LLM the tokens that represented every second frame (a process the authors call temporal sampling), which further halved the input to the LLM.\nResults: STORM outperformed proprietary and open models on measures of video understanding.\nOn MVBench , which asks multiple-choice questions about actions, object interactions, and scene transitions in 16-second videos, STORM achieved 70.6 percent accuracy. That’s better than GPT-4o (64.6 percent accuracy) and Qwen2-VL (67.0 percent accuracy). A baseline system (STORM’s SigLIP and Qwen2-VL LLM without mamba layers, averaging image tokens, and temporal sampling) achieved 69.5 percent.\nOn MLVU , which asks multiple-choice and open-ended questions about videos that range from 3 minutes to over 2 hours long, STORM reached 72.9 percent accuracy, topping GPT-4o (66.2 percent accuracy). The baseline model achieved 70.2 percent.\nWhy it matters: STORM compresses video at the input to the LLM, so the LLM processes 1/8 as many video tokens and uses 1/8 as much compute to process them. This enables the system to work more than 3 times faster than the baseline while performing better.\nWe’re thinking: Initial work on the mamba architecture positioned it as a replacement for the transformer, but this work, along with other projects , combines them to get the benefits of both.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/unnamed--63-.gif",
    "image_filename": "issue-305_better-video-fewer-tokens.gif"
  },
  {
    "issue": 304,
    "title": "Next-Level DeepSeek-R1",
    "url": "https://www.deeplearning.ai/the-batch/issue-304/",
    "content": "DeepSeek updated its groundbreaking DeepSeek-R1 large language model to strike another blow for open-weights performance.\nWhat’s new: The new DeepSeek-R1-0528 surpasses its predecessor and approaches the performance of OpenAI o3 and Google Gemini-2.5 Pro. A smaller version, DeepSeek-R1-0528-Qwen3-8B , runs on a single GPU with as little as 40GB VRAM, according to TechCrunch .\nInput/output: Text in (up to 64,000 tokens), text out (up to 64,000 tokens)\nArchitecture: DeepSeek-R1-0528 mixture-of-experts transformer, 685 billion parameters (upgraded from 671 billion), 37 billion active at any given time; DeepSeek-R1-0528-Qwen3-8B transformer\nFeatures: JSON output, tool use\nAvailability/price: Both models free via Hugging Face for noncommercial and commercial uses under MIT License , DeepSeek-R1-0528 available via DeepSeek’s app by entering the conversation interface and turning on Deep Thinking, DeepSeek API $0.14/$2.19 per 1 million tokens of input/output ($0.035/$0.55 per 1 million tokens of input/output from 4:30 P.M. to 12:30 A.M. Pacific Time)\nUndisclosed: Fine-tuning data and methods\nHow it works: DeepSeek released little information so far about how it built the new models.\nLike the original DeepSeek-R1 , DeepSeek-R1-0528 is a fine-tuned version of DeepSeek-V3 from late 2024. It was exposed to further “algorithmic optimization mechanisms during post-training” and consumes more tokens at inference.\nDeepSeek-R1-0528-Qwen3-8B is based on Qwen3-8B with reasoning knowledge distilled from DeepSeek-R1-0528.\nPerformance: DeepSeek-R1-0528 nips at the heels of top closed LLMs on a variety of benchmarks, while DeepSeek-R1-0528-Qwen3-8B raises the bar for LLMs in its 8-billion-parameter size class. DeepSeek claims general improvements in reasoning, managing complex tasks, and writing and editing lengthy prose, along with 50 percent fewer hallucinations when rewriting and summarizing.\nDeepSeek-R1-0528 improves on the previous version dramatically in some cases. In DeepSeek’s tests, it achieved 17.7 percent of the reasoning problems in HLE compared to the previous version's 8.5 percent. On Aider, it achieved 71.6 percent accuracy compared to the previous version's 53.3 percent accuracy, and it made a similar improvement on AIME 2025 (math) — although it consumed nearly twice as many tokens.\nOn AIME 2024 and AIME 2025 (high-school math competition problems) as well as LiveCodeBench (coding challenges), DeepSeek-R1-0528 performed ahead of Gemini-2.5 Pro-0506 but behind o3. On GPQA Diamond (graduate-level knowledge in a variety of domains), Aider (programming tasks), and HLE (reasoning), it fell behind both Gemini-2.5 Pro-0506 and o3.\nDeepSeek-R1-0528-Qwen3-8B excelled on AIME 2025, where it achieved 76.3 percent, ahead of the much larger Qwen3-32B (72.9 percent) and just behind o3-mini set to medium effort (76.7 percent). It did less well on GPQA, underperforming the other models reported by DeepSeek, and LiveCodeBench, where it fell behind Gemini 2.5-Flash-Thinking-0520.\nBehind the news: The initial version of DeepSeek-R1 challenged the belief that building top-performing AI models requires tens to hundreds of millions of dollars, top-of-the-line GPUs, and enormous numbers of GPU hours. For the second time in less than a year, DeepSeek has built a competitive LLM with a relatively low budget.\nWhy it matters: DeepSeek’s models, along with Alibaba’s Qwen series, continue to narrow the gap between open-weights models and their closed peers. Its accomplishments could lead to wider adoption of less-expensive, more-efficient approaches. DeepSeek is passing along the cost savings to developers, offering high-performance inference at a fraction of the cost of closed models.\nWe’re thinking: DeepSeek-R1-0528-Qwen3-8B mixes contributions from open-weight models — possible only because Qwen3’s license, like DeepSeek’s is permissive. Open models enable experimentation and innovation in ways that closed models do not.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/unnamed--100-.png",
    "image_filename": "issue-304_next-level-deepseek-r1.png"
  },
  {
    "issue": 304,
    "title": "Machine Translation in Action",
    "url": "https://www.deeplearning.ai/the-batch/issue-304/",
    "content": "AI is bringing a massive boost in productivity to Duolingo, maker of the most popular app for learning languages.\nWhat’s new: Duolingo used generative AI to produce 148 courses, more than doubling its previous catalog. The technology enabled the company to offer some of its most popular courses — Spanish, French, German, Italian, Japanese, Korean, and Mandarin — in 28 languages. Initially, the company is using AI to produce courses aimed at beginners, with more advanced levels to come.\nHow it works: Duolingo’s AI-assisted approach to building language courses quickly turns a single course into many. The new approach revved its pace from building 100 courses over 12 years to producing many more than that in less than a year.\nDuolingo starts by building a base course and uses AI to translate it into numerous languages. For example, it can adapt a course that enables English speakers to learn French into a course for Mandarin speakers.\nThe new process gives the company more flexibility in allocating resources, Duolingo’s head of AI Klinton Bicknell told Bloomberg . Previously, the company could dedicate a team to either creating new high-demand courses or updating an existing course. Now it can do both.\nThe quicker pace will enable the company to meet rising demand for instruction in Asian languages such as Japanese, Korean, and Mandarin.\nBehind the scenes: AI is at the heart of Duolingo’s expansion into other areas beyond language learning.\nDuolingo has used OpenAI models to build curricula since 2023. However, it is evaluating models from Anthropic and Google as well as open options.\nFollowing one test, Duolingo concluded that Anthropic’s Claude was “much better” at generating certain types of math content for the company’s relatively new math curriculum, according to Bicknell.\nThe company’s embrace of AI drew criticism last week after CEO Luis von Ahn recently posted on LinkedIn that it would stop hiring contractors to do work that could be automated and increase staffing only in areas that couldn’t be automated. Since then, Duolingo has noted that it plans to hire more engineers and AI researchers, and employees will generate data used to train AI instead of performing quality reviews and other jobs that AI can do faster.\nWhy it matters: Companies in nearly every industry face pressure to produce more with less amid rising competition. AI can help to accomplish that while potentially improving product quality, and Duolingo has ample reason to move aggressively in this direction. The startup Speak , which offers a voice-based approach to learning languages, is growing rapidly, and Google just launched Little Language Lessons that show how an AI-first product could be used as a language teacher and conversational partner.\nWe’re thinking: AI is well on the way to transforming education for teachers, students, and technology companies!",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/unnamed--69-.jpg",
    "image_filename": "issue-304_machine-translation-in-action.jpg"
  },
  {
    "issue": 304,
    "title": "AI Uses Energy, AI Saves Energy",
    "url": "https://www.deeplearning.ai/the-batch/issue-304/",
    "content": "AI’s thirst for energy is growing, but the technology also could help produce huge energy savings over the next five to 10 years, according to a recent report.\nWhat’s new: The International Energy Agency (IEA), which advises 44 countries on energy policy, performed a comprehensive analysis of AI’s energy consumption including energy required to obtain critical materials needed for chips and data centers. The report sees dark clouds ahead but also silver linings.\nDark clouds: The report, which is based on interviews with officials in government, energy, and technology, makes four projections for AI’s energy consumption. In the base scenario, future growth and efficiency gains are similar to those of the past five years. The agency also plots a “take-off” scenario in which AI adoption happens faster, a “high efficiency” scenario with lower energy needs, and a “headwinds” scenario in which adoption of AI slows or infrastructure bottlenecks impede construction. Among the conclusions:\nDemand for electricity by data centers worldwide will more than double by 2030 in the base scenario, growing from 415 terawatt-hours (TWh) today to 945 TWh, around 2.5 percent of current global energy consumption. By 2035, this figure will range from 700 TWh to 1700 TWh.\nBy 2030, data centers outfitted with AI accelerator chips will consume four times the energy they do today.\nThe United States, China, and Europe have more data centers (and use more electricity) than the rest of the world. Like many countries, their data centers are in a few geographic regions, drawing from the same power sources, which eventually will strain local electrical grids. Together, the U.S. and China will account for 80 percent of global growth in data center electricity consumption by 2030. Japan and Malaysia will also see strong growth.\nSilver linings: AI already makes energy generation, distribution, and use more efficient. The authors expect these savings to accelerate.\nExisting AI algorithms predict energy generation and consumption. This makes it easier to integrate renewable energy sources into the grid, which reduces reliance on fossil fuels and cuts the resulting pollutants and greenhouse gases. Extending existing programs to increase use of renewables by 1 percent would reduce CO2 emissions by 120 megatons by 2035, which is roughly 40 percent of the projected emissions attributable to data centers.\nWidespread adoption of existing AI applications that streamline energy consumption in industry, transportation, and buildings could reduce CO2 emissions by 1.4 gigatons, nearly five times the projected emissions attributable to data centers, by 2035. For example, scaling up existing AI optimization of heating, ventilation, and air-conditioning systems would save 300 TWh, about one-third of total energy used by data centers.\nAI and cloud-computing companies continue to negotiate long-term purchase agreements that can secure renewable and zero-emissions energy for as much as 20 years. Data center operators are responsible for most of the long-term contracts that have been announced, nearly all of them for solar energy. Consequently, renewables generation is projected to grow by over 450 TWh by 2035.\nThe energy costs of training, inference, and cooling hardware are expected to fall further thanks to trends in AI models (fewer parameters, more efficient algorithms, task-specific models) hardware (more energy-efficient chips, improved cooling methods), and usage (batch processing, running smaller models locally rather than in the cloud).\nYes, but: The authors concede that lower energy costs for AI likely will lead to much greater consumption — according to the Jevons paradox — so more-efficient models and hardware will result in higher energy consumption overall.\nBehind the news: Data centers were growing rapidly prior to the boom in generative AI. Data centers’ electricity use doubled between 2000 and 2005 and again between 2017 and 2022, driven by the growth of cloud computing and data storage, streaming and social media, and cryptocurrency mining. However, these periods of accelerating growth were followed by periods of slower growth as efforts to cut costs led to more-efficient software and hardware. The authors expect this pattern to hold.\nWhy it matters: The IEA report is a first-of-its-kind analysis of AI’s energy requirements, how they’re likely to grow, as well as the potential of the technology itself to reduce those requirements. It confirms that AI is poised to consume huge amounts of energy. However, it also suggests that today’s energy costs will be tomorrow’s energy savings as AI makes energy generation, distribution, and use more efficient across a wide variety of industries.\nWe’re thinking: While demand for electricity for data centers is growing rapidly, calibrating the right level of investment is tricky. High levels of growth come with high levels of hype that can lead analysts to overestimate future demand. For example, Microsoft, after examining its forecasts, canceled data-center projects that would have consumed 2 gigawatts.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/unnamed---2025-06-04T165349.311.png",
    "image_filename": "issue-304_ai-uses-energy-ai-saves-energy.png"
  },
  {
    "issue": 304,
    "title": "Phishing for Agents",
    "url": "https://www.deeplearning.ai/the-batch/issue-304/",
    "content": "Researchers identified a simple way to mislead autonomous agents based on large language models.\nWhat’s new : Ang Li and colleagues at Columbia University developed a method to exploit the implicit trust that agents tend to place in popular websites by poisoning those websites with malicious links.\nKey insight : Commercially available agentic systems may not trust random sites on the web, but they tend to trust popular sites such as social-media sites. An attacker can exploit this trust by crafting seemingly typical posts that link to a malicious website. The agent might follow the link, mistakenly extending its trust to an untrustworthy site.\nHow it works : The authors tested web-browsing agents including Anthropic Computer Use and MultiOn on tasks such as shopping or sending emails.\nThe authors created Reddit posts that aligned thematically with a particular agentic task, such as shopping for Air Jordan 1 shoes. The posts contained text akin to marketing (for example, “Where to Buy Air Jordan 1 Chicago”) as well as instructions that pointed to a malicious site controlled by the authors (“for more information, check out <website>”).\nThe authors fed a query like “Where can I buy Nike Air Jordan 1 in Chicago?” to the agent. They also entered sensitive information like credit card details or email credentials.\nThe agent searched the web for resources needed to fulfill the query. It examined sites and found the Reddit posts written by the authors.\nThe agent followed the instructions in the posts and visited the malicious website. The website included instructions that manipulated the agent to pursue an attacker’s goal, such as submitting credit card information or sending phishing emails from the user’s email address.\nResults : Once an agent was redirected to the malicious websites, it reliably followed the attacker’s instructions. For example, each of the agents tested divulged credit card information in 10 out of 10 trials. Similarly, each agent sent a phishing message from the user’s email account asking recipients to send money to a malicious “friend” in 10 out of 10 trials.\nWhy it matters : Giving agents the ability to perform real-world actions, such as executing purchases and sending emails, raises the possibility that they might be tricked into taking harmful actions. Manipulating agents by referring them to malicious web content is an effective vector of attack. Agents will be more secure if they’re designed to avoid and resist such manipulation.\nWe’re thinking: Humans, too, can be fooled by phishing and other malicious activities, and the path to programming agents to defend against them seems easier than the path to training the majority of humans to do so. In the long term, agents will make online interactions safer.\nStay updated with weekly AI News and Insights delivered to your inbox\nCourses\nThe Batch\nCommunity\nCareers\nAbout",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/06/unnamed---2025-06-04T165354.442.png",
    "image_filename": "issue-304_phishing-for-agents.png"
  },
  {
    "issue": 303,
    "title": "Claude 4 Advances Code Generation",
    "url": "https://www.deeplearning.ai/the-batch/issue-303/",
    "content": "Anthropic continued its tradition of building AI models that raise the bar in coding tasks.\nWhat’s new: Anthropic launched Claude 4 Sonnet 4 and Claude Opus 4 , the latest medium- and largest-size members of its family of general-purpose large language models. Both models offer an optional reasoning mode and can use multiple tools in parallel while reasoning. In addition, the company made generally available Claude Code, a coding agent previously offered as a research preview, along with a Claude Code software development kit.\nInput/output: Text, images, PDF files in (up to 200,000 tokens); text out (Claude Sonnet 4 up to 64,000 tokens, Claude Opus 4 up to 32,000 tokens)\nFeatures: Parallel tool use including computer use, selectable reasoning mode with visible reasoning tokens, multilingual (15 languages)\nPerformance: Ranked Number One in LMSys WebDev Arena, state-of-the-art on SWE-bench and Terminal-bench\nAvailability/price: Anthropic API, Amazon Bedrock, Google Cloud Vertex AI. Claude Sonnet 4 $3/$15 per million input/output tokens, Claude Opus 4 $15/$75 per million input/output tokens\nUndisclosed: Parameter counts, specific training methods and datasets\nHow it works: The team trained the Claude 4 models on a mix of publicly available information on the web as well as proprietary purchased data, data from Claude users who opted to share their inputs and outputs, and generated data. They fine-tuned the models to be helpful, honest, and harmless according to human and AI feedback .\nThe models make reasoning tokens visible within limits. For especially lengthy chains of thought, an unspecified smaller model summarizes reasoning tokens.\nGiven local file access, Claude Opus 4 can create and manipulate files to store information. For instance, prompted to maintain a knowledge base while playing a Pokémon video game, the model produced a guide to the game that offered advice such as, “If stuck, try OPPOSITE approach” and “Change Y-coordinate when horizontal movement fails.”\nResults: Both Claude 4 models tied Google Gemini 2.5 Pro at the top of the LMSys WebDev Arena and achieved top marks for coding and agentic computer-use benchmarks in Anthropic’s tests.\nOn SWE-bench Verified , which tests the model’s ability to solve software issues from GitHub, Claude Opus 4 succeeded 72.5 percent of the time, and Claude Sonnet 4 succeeded 72.7 percent of the time. The next best model, OpenAI o3, succeeded 70.3 percent of the time.\nTerminal-bench evaluates how well models work with the benchmark’s built-in agentic framework to perform tasks on a computer terminal. Claude Opus 4 succeeded 39.2 percent of the time and Claude Sonnet 4 succeeded 33.5 percent of the time, whereas the closest competitor, OpenAI GPT 4.1, succeeded 30.3 percent of the time. Using Claude Code as the agentic framework, Claude Opus 4 succeeded 43.2 percent of the time and Claude Sonnet 4 succeeded 35.5 percent of the time.\nWhy it matters: The new models extend LLM technology with parallel tool use, using external files as a form of memory, and staying on-task over unusually long periods of time. Early users have reported many impressive projects, including a Tetris clone built in one shot and a seven-hour stint refactoring Rakutan’s open-source code base .\nWe’re thinking: Prompting expert @elder_plinius published a text file that is purported to be Claude 4’s system prompt and includes some material that does not appear in Anthropic’s own publication of the prompts. It is instructive to see how it conditions the model for tool use, agentic behavior, and reasoning.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/05/unnamed--97-.png",
    "image_filename": "issue-303_claude-4-advances-code-generation.png"
  },
  {
    "issue": 303,
    "title": "Google I/O Overdrive",
    "url": "https://www.deeplearning.ai/the-batch/issue-303/",
    "content": "Google revamped its roster of models, closed and open, and added more AI-powered features to its existing products.\nWhat’s new: Google staged a parade of announcements at this year’s I/O developer conference. New offerings include improvements to Gemini 2.5 Pro and Gemini 2.5 Flash and a preview of Gemma 3n (all three generally available in June), the updated Veo 3 video generator (available via Flow, Google’s AI videography app, for paid subscribers to its AI Pro and Ultra services), and increasingly AI-powered search.\nHow it works: The I/O offerings spanned from public-facing products to developer tools.\nGoogle updated Gemini 2.5 Pro and the speedier Gemini 2.5 Flash with audio output, so both models now take in text, audio, images, and video and produce text and audio. In addition, they offer summaries of tokens produced while reasoning. Gemini-2.5-Pro-Preview-05-06, which topped the LMSys Text Arena and WebDev Arena (tied with Claude 4 Opus and Sonnet), lets users set a reasoning budget up to 128,000 tokens, enabling it to outperform OpenAI o3 and o4-mini (set to high effort) on math, coding, and multimodal benchmarks in Google’s tests. Gemini-2.5-Flash-Preview-05-20 uses 22 percent fewer tokens than its predecessor while ranking near the top of the LMSys Text Arena and WebDev Arena.\nThe Veo 3 text-to-video generator produces 3840x2160-pixel video with audio (dialogue, sound effects, and music) with creative controls including the ability to add and remove objects and maintain consistent characters. It bested Kuaishu Kling 2.0, Runway Gen 3, and OpenAI Sora in Google’s comparisons.\nNew members of Google’s Gemma 3 family of open-weights models, Gemma 3n 5B and 8B, are multilingual (over 140 languages), multimodal (text, vision, audio in; text out), and optimized for mobile platforms. Gemma-3n-E4B-it (8 billion parameters) ranks just ahead of Anthropic Claude 3.7 Sonnet in the LMSys Text Arena. Gemma 3n 5B and 8B are 1.5 times faster than their predecessors and require 2 gigabytes and 3 gigabytes of memory, respectively, thanks to techniques that include per-layer embeddings, key-value caching, conditional parameter loading (constraining active parameters to specific modalities at inference), and a Matryoshka Transformer design that dynamically activates nested sub-models. They’re available in preview via Google’s AI Studio, AI Edge, GenAI SDK, or MediaPipe.\nGoogle introduced several specialized AI tools and models. Jules is an autonomous, asynchronous, multi-agent coding assistant that clones repos into a secure virtual machine to perform tasks like writing tests, building features, and fixing bugs (available in public beta). SignGemma translates American sign language to text (previously ASL to English). MedGemma analyzes medical text and images (part of the open-weights collection Health AI Developer Foundations).\nBuilding on Google Search’s AI Overviews, Google is further building AI into search. Google Search’s AI Mode uses Gemini 2.5 to deliver a “deep search” mode that decomposes users’ questions into hundreds of sub-queries for analysis and visualization. Google plans to integrate AI Mode features into its core search product. In addition, Google Search’s AI Mode will gain Search Live (real-time, audio-enabled visual interaction via camera) and agentic features (for tasks such as purchasing tickets). Computer-use capabilities are coming to the Gemini API and Vertex AI.\nWhy it matters: Google is catching up with the Microsoft/OpenAI colossus on several fronts. The addition of audio output to Gemini and Gemma models fuels the rise of voice-to-voice and other audio applications and gives developers powerful new tools to build them. At the same time, Veo 3’s text-to-video-plus-audio output shows marked improvement over the previous version.\nBehind the news: The number of tokens Google processed monthly has surged this year from 9.7 trillion last year to 480 trillion, a sign that its AI APIs and AI-infused products are rapidly gaining traction. Google’s progress contrasts with Apple’s ongoing struggles . Both share advantages in smartphones and app distribution. But, while Google has showcased a string of advanced models as well as early efforts to integrate them into legacy products, Apple’s organizational challenges have hampered its AI development. Now Apple must contend with OpenAI’s acquisition of LoveFrom, the startup founded by its former lead product designer Jony Ive.\nWe’re thinking: Google I/O 2025 was a strong showing of generative AI capabilities! There’s still work to be done to translate these innovations into compelling products, but the company now has a strong base for building numerous innovative products.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/05/unnamed--60-.gif",
    "image_filename": "issue-303_google-i-o-overdrive.gif"
  },
  {
    "issue": 303,
    "title": "How DeepSeek Did It",
    "url": "https://www.deeplearning.ai/the-batch/issue-303/",
    "content": "DeepSeek made headlines late last year, when it built a state-of-the-art, open-weights large language model at a cost far lower than usual. The upstart developer shared new details about its method.\nWhat’s new: Chenggang Zhao and colleagues at DeepSeek described software and hardware choices that reduced memory and processing requirements while building their groundbreaking mixture-of-experts models DeepSeek-R1 and DeepSeek-V3.\nMixture of experts (MoE) basics: The MoE architecture uses different subsets of a model’s parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a routing module that learns to choose which one(s) to use based on the training example. In this way, different experts learn to specialize in different types of input.\nHow it works: The authors trained DeepSeek-R1 and DeepSeek-V3 using a cluster of 2,048 Nvidia H800 GPUs composed of nodes that contained 8 GPUs each. MoE requires less memory than dense architectures, since a given input activates only a portion of a model’s parameters. This enabled the authors to train DeepSeek-V3 on 250 GFLOPs per token, while Qwen 2.5 72B required 394 GFLOPs per token and Llama 3.1 405B required 2,448 GFLOPs per token.\nThe authors built a mixed-precision training algorithm to reduce the memory requirements of training MoE models. They used FP8 (8-bit) numbers to perform computations including linear transformations and 16- or 32-bit precision to perform others such as computing embeddings. (They say DeepSeek-V3 was the first open LLM to have been trained using FP8.)\nThe authors noticed that communication between GPUs inside a node was four times faster than communication between nodes. To ensure fast communication when routing tokens to experts, they limited the algorithm to process them within up to 4 nodes.\nTo utilize GPUs more fully, they divided each GPU’s input data so the chip processes computation and communication at the same time. Specifically, the chip computes attention or MoE layers on one part of the data and simultaneously sends the other part of the data to other GPUs or aggregates it from other GPUs as necessary.\nTo further save inference memory, the models use multi-head latent attention, which saves memory during execution relative to other variants of attention. The authors compared their implementation to the variant GQA used in Qwen 2.5 72B and Llama 3.1 405B. Their method (70 kilobytes per token) used far less memory than Qwen-2.5 (328 kilobytes per token) or Llama 3.1 (516 kilobytes per token).\nBehind the news: DeepSeek-V3 made waves when it was released in December. It performed better than Llama 3.1 405B, the leading LLM at the time, but its training cost was an astonishing $5.6 million, compared to the usual tens to hundreds of millions of dollars. Some observers were skeptical of the reported cost, pointing out that the $5.6 million dollar figure doesn’t include salaries, data acquisition and annotation, processing failed training runs, and other research and development costs. In addition, the cost of training DeepSeek-R1 remains unknown.\nWhy it matters: Traditionally, only companies with large budgets and vast resources could afford to train state-of-the-art models. DeepSeek changed that but didn’t explain how when it released its models. By sharing the details, the company has empowered a wider range of teams to improve the state of the art.\nWe’re thinking: Shortly after DeepSeek-R1 was released, some engineers claimed — without presenting evidence — that DeepSeek had copied their work. DeepSeek’s disclosure of its training methods should lay to rest any remaining questions about this. Its work was truly innovative, and we applaud its release of key technical details.",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/05/unnamed--98-.png",
    "image_filename": "issue-303_how-deepseek-did-it.png"
  },
  {
    "issue": 303,
    "title": "Did GPT-4o Train on O’Reilly Books?",
    "url": "https://www.deeplearning.ai/the-batch/issue-303/",
    "content": "A study co-authored by tech-manual publisher Tim O’Reilly shows that OpenAI trained GPT-4o on parts of his company’s books that were not made freely available.\nWhat happened: O’Reilly, computer scientist Sruly Rosenblat, and economist Ilan Strauss found that GPT-4o was able to identify verbatim excerpts from dozens of O’Reilly Media books that the company kept behind a paywall, indicating that the books likely were included in the model’s training data.\nHow it works: The researchers adapted the DE-COP method to compare how well GPT-4o, GPT-4o-mini, and GPT-3.5 Turbo recognized paywalled excerpts versus freely available excerpts from the same books.\nThe team selected 34 O’Reilly Media books and divided them into roughly 14,000 paragraphs.\nThey labeled the paragraphs private (paywalled) or public (when O’Reilly Media publishes a book, it distributes freely on the web chapters 1 and 4 as well as the first 1,500 characters of other chapters). They also labeled the paragraphs according to whether they were published before or after the models’ knowledge cutoff dates.\nThe team built multiple-choice quizzes, each composed of a verbatim paragraph and three paraphrased versions generated by Claude 3.5 Sonnet. The researchers ordered the paragraphs and paraphrases in all permutations to eliminate potential position bias.\nResults: The authors asked each model to identify the verbatim paragraph and calculated each model’s percentage of correct responses. Then they averaged each model’s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially included in the training set) from those that weren’t available at the time. 50 percent AUROC indicates random chance, while higher scores indicate higher accuracy.\nGPT-4o tended to recognize O’Reilly Media content whether or not it was public, but it recognized private paragraphs (82 percent AUROC) markedly more often than public paragraphs (64 percent AUROC).\nGPT-4o-mini’s performance was nearly random for both private (56% AUROC) and public material (55% AUROC). The researchers hypothesize that either (i) the model’s smaller size may limit its ability to memorize or (2) OpenAI may reserve premium data to train larger models.\nThe earlier GPT-3.5 Turbo recognized public paragraphs (64% AUROC) more often than private paragraphs (54% AUROC), which suggests that it was trained predominantly on freely available data.\nYes, but: Newer large language models are better at distinguishing human-written from generated text, even if it wasn’t in their training sets. For instance, given paragraphs that were published after their knowledge cutoffs, GPT-4o returned scores as high as 78 percent AUROC. The authors note that this may challenge their conclusions, since they interpret high scores to indicate that a model saw the text during training. Nonetheless, they argue that their approach will remain valid while scores for both text that was included and text that was excluded from training sets remain under 96 percent AUROC. “For now,” they write, “the gap remains sufficiently large to reliably separate the two categories.”\nBehind the news: Historically AI developers have trained machine learning models on any data they could acquire. But in the era of generative AI, models trained on copyrighted works can mimic the works and styles of the works’ owners, creating a threat to their livelihoods. Some AI developers have responded by regarding data that’s freely available on the web as fair game, and material that’s otherwise protected as off-limits for training. However, datasets that include ostensibly private data are widely circulated, including LibGen , which includes all 34 of the O’Reilly Media titles tested in this study. Moreover, unauthorized copies of many copyrighted works are posted without paywalls or even logins, making it possible even for web scrapers that crawl only the open web to download them. Google and OpenAI, which is currently embroiled in lawsuits by authors and publishers who claim it violated copyrights by training models on copyrighted works, recently lobbied the United States government to relax copyright laws for AI developers.\nWhy it matters: The AI industry requires huge quantities of high-quality data to keep advancing the state of the art. At the same time, copyright owners are worried that models trained on their data might hamper their opportunities to earn a living. AI developers must find fair ways to respond. As O’Reilly points out, exploiting copyrighted works instead of rewarding their authors could lead to an “extractive dead end” that ultimately diminishes the supply of the high-quality training data.\nWe’re thinking: We have learned a great deal from O’Reilly Media’s books, and we’re grateful to the many authors, editors, graphic artists, and others who produce them. Meanwhile, it’s time for the U.S. Congress —  and legislators internationally — to update copyright laws for the era of generative AI, so everyone knows the rules and we can find ways to follow them.\nStay updated with weekly AI News and Insights delivered to your inbox\nCourses\nThe Batch\nCommunity\nCareers\nAbout",
    "image_url": "https://charonhub.deeplearning.ai/content/images/2025/05/unnamed--99-.png",
    "image_filename": "issue-303_did-gpt-4o-train-on-o-reilly-books.png"
  }
]